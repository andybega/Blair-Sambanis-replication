---
title: "How much variance in AUC-ROC is there between model runs?"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Since random forests are non-deterministic models, if you don't set a RNG seed the results for identical input will vary each time due to slightly different initial conditions. This means that the predictions and thus AUC-ROC values will also differ to some extent between model runs. Setting the RNG seed allows exact reproduction, but it doesn't solve this variability due to RNG state. We don't want results to be sensitive to a particular RNG seed. So, how much does AUC-ROC vary between model runs?

This script will run the base specification escalation model several times, in both the 1-month and 6-month versions, and then check the distribution of test AUC-ROC values obtained. 

The chunk below will run the models. It's cached by default because this can take more than a few seconds to run, depending on the number of samples. 

- `WORKERS`: how many parallel workers to use; number of cores or one less usually is good
- `SAMPLES`: how many times to run the model for each of the two data versions

```{r packages}
# Don't cache this, otherwise the chunks behind run-models will not work
suppressMessages({
  library(dplyr)
  library(readr)
  library(randomForest)
  library(pROC)
  library(foreach)
  library(doFuture)
  library(tidyr)
  library(ggplot2)
})
```


```{r run-models, cache=TRUE}
WORKERS  <- 8
SAMPLES  <- 40

# Function to wrap the AUC-ROC calculation
auc_roc_vec <- function(pred, truth, smooth) {
  roc_obj <- pROC::roc(truth, pred, auc = TRUE, quiet = TRUE, smooth = smooth)
  as.numeric(roc_obj$auc)
}

features <- c("gov_opp_low_level", "gov_reb_low_level", "opp_gov_low_level",
              "reb_gov_low_level", "gov_opp_nonviol_repression", "gov_reb_nonviol_repression",
              "gov_opp_accommodations", "gov_reb_accommodations", "reb_gov_demands",
              "opp_gov_demands")

train_end_year <- 2007

hp_settings <- list(ntree = 100000L, maxnodes = 5L, sampsize = 100L, replace = FALSE)

dv_name <- "incidence_civil_ns_plus1"


data_1mo <- read_rds("trafo-data/1mo_data.rds")
data_6mo <- read_rds("trafo-data/6mo_data.rds")

registerDoFuture()
plan("multisession", workers = WORKERS)

model_table <- crossing(
  horizon = rep(c("1 month", "6 months"), SAMPLES),
  sample  = rep(1:SAMPLES, each = 2)
)

# shuffle the table so the work load on average is more evenly distributed 
# among workers
model_table <- model_table[sample(1:nrow(model_table)), ]

res <- foreach(
  i = 1:nrow(model_table),
  .inorder = FALSE
) %dopar% {

  horizon_i <- model_table[i, ][["horizon"]]
  sample_i  <- model_table[i, ][["sample"]]

  if (horizon_i=="1 month") {
    data_i <- data_1mo
  } else {
    data_i <- data_6mo
  }

  train_data <- data_i %>% filter(year <= train_end_year)
  test_data  <- data_i %>% filter(year >  train_end_year)

  # randomForest with the x, y instead of formulate interface will not drop
  # missing values, so we need to make sure the training data drops those
  # manually
  train_data   <- train_data %>%
    dplyr::select(one_of(c(dv_name, features))) %>%
    filter(complete.cases(.))

  fitted_model <- suppressWarnings(randomForest(
    x = train_data[, features],
    y = train_data[, dv_name],
    ntree    = hp_settings$ntree,
    maxnodes = hp_settings$maxnodes,
    sampsize = hp_settings$sampsize,
    replace = FALSE, do.trace = FALSE, importance = FALSE
  ))

  # Because the original rep code uses the formula interface, the predictions
  # it puts out cover all of the test data, with NA values where needed for
  # incomplete rows. Replicate that here.
  non_missing_test_data <- test_data %>%
    dplyr::select(one_of(c("year", "period", "country_iso3", features))) %>%
    dplyr::filter(complete.cases(.))
  preds <- non_missing_test_data %>%
    dplyr::select(year, period, country_iso3) %>%
    mutate(pred = predict(fitted_model, newdata = non_missing_test_data,
                          type = "response")) %>%
    # add the non-complete rows back in
    right_join(test_data %>%
                 dplyr::select(year, period, country_iso3, one_of(dv_name)),
               by = c("year", "period", "country_iso3")) %>%
    # the original rep scripts do this; doesn't make a difference for ROC
    # calculations
    mutate(pred2 = ifelse(is.na(!!sym(dv_name)), NA_real_, pred)) %>%
    arrange(country_iso3, year, period)

  res_i <- model_table[i, ] %>%
    mutate(
      auc_roc          = auc_roc_vec(pred = preds$pred, truth = preds[, dv_name],
                                     smooth = FALSE),
      auc_roc_smoothed = auc_roc_vec(pred = preds$pred, truth = preds[, dv_name],
                                     smooth = TRUE),
      # The number of non-missing rows in the training data used to fit the mdl
      N_train = nrow(train_data),
      # The number of non-missing predictions
      N_test  = sum(complete.cases(preds))
    )
  res_i
}

res <- bind_rows(res)
write_rds(res, "output/variance.rds")
```

```{r}
res <- read_rds("output/variance.rds")
res_long <- res %>%
  dplyr::select(-N_train, -N_test) %>%
  tidyr::pivot_longer(auc_roc:auc_roc_smoothed, names_to = "smoothed", values_to = "auc_roc") %>%
  mutate(smoothed = ifelse(smoothed=="auc_roc", FALSE, TRUE)) 

res_tbl <- res_long %>%
  group_by(horizon, smoothed) %>%
  dplyr::summarize(N = n(),
                   mean = mean(auc_roc),
                   min  = min(auc_roc),
                   max  = max(auc_roc),
                   sd   = sqrt(mean((auc_roc-mean(auc_roc))^2)), # pop sd, not sample sd
                   .groups = "drop")
res_tbl %>%
  knitr::kable(digits = 3)
```

Comparing across the two dimensions--horizon and smoothed--actually smoothing has more impact on the results than the forecast horizon. 

```{r}
# I want to add horizontal lines at the boundaries for rounding AUC-ROC to 
# two digits. Since the plot below is faceted, need to use a separate data frame
# for this with facet variable
hlines <- bind_rows(
  tibble(smoothed = "Smoothed", y = c(.82, .83, .84, .85) + .005),
  tibble(smoothed = "Original", y = c(.77, .78) + .005)
)

res_long %>%
  mutate(smoothed = ifelse(smoothed, "Smoothed", "Original"),
         horizon  = factor(horizon), 
         smoothed = factor(smoothed, levels = c("Smoothed", "Original"))) %>%
  ggplot() +
  facet_wrap(~smoothed, scales = "free") +
  geom_violin(aes(x = horizon, y = auc_roc)) +
  geom_point(aes(x = horizon, y = auc_roc), alpha = .5) +
  geom_hline(data = hlines, aes(yintercept = y), linetype = 3) +
  theme_bw()
```

The dotted lines in the graph mark the boundaries when rounding AUC-ROC to two digits. So for example, the original 1-month AUC-ROC straddles a boundary, meaning it is likely that the last digit in the tables reported in the paper will flip between model runs. On the other hand, the original 6-months version is between boundaries so the rounded results are likely to not change between runs. 

But this straddling of rounding points aside, it seems that the variance of AUC-ROC values is generally pretty low: \(2 \times \textrm{SD} \lesssim 0.01 \). Not enough to affect results interpretation as long as we disregard differences of 0.01 or less. At least based on the escalation model, it seems some of the models in Table 2 might vary a bit more. 
