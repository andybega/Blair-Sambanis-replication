---
title: "Modified replication"
output: 
  github_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE)

library(dplyr)
library(readr)
```

The code in this folder replicates the results in Tables 1 and 2 of B&S, but does so with both the smoothed ROC curve AUC values B&S report, and regular, non-smoothed ROC curve AUC. 

In order to be able to run the models needed to create tables 1 and 2 more quickly than with the original replication code, the code here has been re-factored to enable running in parallel. The structure of the code has completely changed. Instead of a master script that handles file IO and sources other scripts to setup model definition objects (features, training/test data frames, etc.) and run models, the new logic is to:

1. Create a central model table that encodes all cells in B&S Tables 1 and 2 and the information needed to define the underlying models, as well as associated model definitions like features, hyper-parameter settings, etc. (`output/model-definitions/`). 
2. Run through the model table (mostly in parallel) to estimate all models; save the resulting predictions. 
3. Re-create output tables and figures. 

## Reproducing our replication

Below are instructions for how to run the script in order to replicate the updated analysis. To do a clean replication, delete the `output/` folder; this is not required though. 

The file numbering indicates the order in which files should be run, and correspond to the steps above. Files with the same number can be run in any order, as long as lower number files have already been run. 

Setup:

- `0-packages.R` makes sure all required R packages are installed. 

Preparing data for model estimation:

- `1-re-save-data.R` changes the format of the saved 1-month and 6-month data versions from Stata 13 .dta files to R .rds format. This can be read much quicker than the Stata format. 
- `1-make-original-tables.R` is a by-hand coding of the values reported for Tables 1, 2, and 4 in B&S 2020. 
- `1-setup-model-table.R` encodes all the various pieces of information needed to actually run the models reflected in tables 1 and 2. In the original replication files this is done in bits and pieces in `+master.R`, `code/[1|6]mo_define_models.R`, and each model runner file, e.g. `code/1mo_run_escalation.R`. The primary output are several files written to the `output/model-definitions` folder. More details on this below.

Estimate all models (this is where the heavy lifting takes places):

- `2-model-runner.R` runs the models. Adjust the number of WORKERS at the top of the script as needed based on the number of cores on your computer. The primary output of this script is `output/model-table-w-results.csv` and `output/all-predictions.R`.

Process results; figures, tables, etc.:

- `3-...`: the various scripts starting with "3-" recreate the various tables and figures that end up in the paper. These are written to the `rep_nosmooth/output` folder here; the `paper/` folder includes a `update-inputs.R` script that will copy needed pieces over to that folder. 

In addition, there are some ancillary materials that are not required for replicating the results in the paper, but add additional information. These are easiest to read in GitHub.  

- [replication-check.md](replication-check.md) compares the AUC-ROC results we obtain when using smoothed ROC curves to the original B&S results (hand-copied from the paper). When taking into account the implementation fixes we made for two models, and RNG seed variation, we are able to replicate the original results/patterns. 
- [variance.md](variance.md) examines how much the AUC-ROC results vary when you don't set an RNG seed. For the base specification escalation model, the variance is significantly below 0.01, and small changes in the Table 1 and 2 replications mainly occur when the distribution straddles a threshold used for rounding, e.g. 0.825. 
- `rep-check.R` compares the results of our modified replication to the original values reported in Tables 1 and 2 of the B&S paper. 

## Model definitions

The original replication code has a large number of scripts that define the various models/methods to run. In order to allow running the RF models in parallel, this refactored code sets up a table and other lists that define all the information needed to run the various models, and then runs through the table (in parallel) to run each model. 

The various models going into tables 1 and 2 vary over several dimensions. We started untangling this by first encoding every AUC-ROC value/cell in Tables 1 and 2. These can be uniquely identified by the forecast horizon (1 or 6-months), and then the table row and columns. There are a total of 100 AUC-ROC values reported, 90 in Table 1, and 10 in Table 2. 

_(The "Escalation only" values in Table 2 actuall correspond to the models that produce the \["Base specification", "Escalation"\] values in Table 1, but it is easier to duplicate them since the underlying models in any case estimate pretty quickly.)_

One of the principal outputs of `setup-model-definitions.R` is this encoding, in `output/model-definitions/model-table.rds`:

```{r, echo = TRUE}
model_table <- read_rds("output/model-definitions/model-table.rds")
head(model_table)
```

The majority of cells correspond to underlying random forest (RF) models--78 of 100 entries. For these RF cells, the table columns correspond to the feature specification going into the model as predictors. This mapping is encoded in `output/model-definitions/feature-specs.json`. 

Forecasting horizon aside, the rows encode three dimensions of info:

- the DV to use (`dv-specs.yaml`), specifically either the regular one or one of the two alternate codings
- the train/test split to use (`train-end-year.yaml`), one of 4 possible values
- the RF hyperparameter settings to use (`hp-settings.yaml`). 

Together these bits define all that we need to run the correct random forest models for these 78 cells. 

The other 22 cells mostly involve averaging predictions produced by the other models. For example, the "Average" column in Table 1 is for a model that averages the predictions from the other four models(/specifications). This doesn't require running any model per se; just averaging existing predictions. These are handled via special treatment in `model-runner.R`, outside the main parallel loop. 


