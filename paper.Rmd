---
title: "Comments on Blair and Sambanis, 2020, \"Forecasting Civil Wars: Theory and Structure in an Age of 'Big Data' and Machine Learning\", *JCR*"
author: 
 - Andreas Beger^[Predictive Heuristics, adbeger@gmail.com. corresponding author]
 - Richard K. Morgan^[V-Dem, rick.morgan2@gmail.com.]
 - Michael D. Ward^[Predictive Heuristics, Duke University, University of Washington, michael.don.ward@gmail.com]
date: "`r format(Sys.time(), '%d %B %Y')`"
thanks: Cassy L. Dorff and Shahryar Minhas both provided helpful feedback on this project. All the code and several additional analyses can be found at our replication archive at https://github.com/rickmorgan2/Blair-Sambanis-replication.
abstract: We examine the research protocols in Blair and Sambanis (2020).  We find that there are several important mistakes and research decisions that determine their results. Fixing these mistakes results in a reversal of their claim that theory based models of escalation are better at predicting onsets of civil war than other kinds of models.  While their model is not very theoretical, it is outperformed by several of the ad hoc, putatively non-theoretical models they devise and examine.  
output: 
  bookdown::pdf_document2:
    keep_tex: true
    keep_md: true
    toc: false
# spacing: double
bibliography: 
  - "`r user <- Sys.info()[['user']]; if (user=='mdw') '/Users/mdw/Git/whistle/master.bib' else 'references.bib'`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE)
# options(tinytex.verbose = TRUE)

library(bookdown)
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(kableExtra)
library(stringr)
```
# Introduction
 @blair:sambanis:2020 (hereafter B\&S) argue that theory is essential for creating models that have high accuracy in forecasting civil war onset. Indeed they assert that with such theory, forecasting is more accurate than has previously been possible. Setting aside the validity of this argument, we re-examine the empirical basis for the claims made in their article. We find that these claims are false. Their theory-based escalation model does not do better than the alternatives that they examine. Indeed, it does worse. The reason for this reversal of their conclusion is that they have made several mistakes in their research procedure. The performance results they report are based on smoothed performance curves, not the original unsmoothed curves. Two of the structural alternatives to their basic escalation model were incorrectly implemented. We also found that the scoring of their forecasts for the first half of 2016 were incorrectly performed using civil war incidence, not onset. In what follows, we show the impact of these mistakes on the conclusions. 

B\&S claim (page 3) to show that a model informed by procedural theories of escalation and de-escalation can predict the onset of civil wars ``remarkable accurately.'' Indeed they argue that this so-called theoretical model outperforms four other ``more mechanical'' alternatives. Second, they claim that the integration of structure with process is better over short forecasting windows. Third, they preregistered the list of thirty countries that have the highest risk of civil war onset. They claim that such prospective predictions are rare in the literature when, in fact, they have been routine for many years with several prominent projects. B\&S claim to be unique in assessing these forecasts. A qualitative analysis of their predictions allows them to conclude that their model is robust, and a more precise test is undertaken. We will return to their analysis later, after correcting the procedural mistakes we found in their research process.

Before proceeding, we quote B&S (page 24):

> Our theoretically driven model generates accurate forecasts, with base specification AUCs of 0.82 and 0.85 over one- and six-month windows, respectively, and AUCs as high as 0.92 in other specifications. Our model also consistently and sometimes dramatically outperforms the alternatives we test... Cederman and Weidmann (2017, 476) argue that "the hope that big data will somehow yield valid forecasts through theory-free 'brute force' is misplaced in the area of political violence." Our results lend some credence to this claim.

# Summary of Blair and Sambanis (2020)

Blair and Sambanis (2020) (B\&S hereafter) aim to examine whether 
theory adds more value to a forecasting model when compared to non-parametric machine learning models, and specifically random forests, whose specifications are not theory-informed. For the moment, we set aside a) the logic of this hypothesis and b) whether their model has more theory than is typically found in empirical conflict models. In short, they uphold their assumption that theory-guided empirical research produces better conflict predictions than machine learning inspired efforts that are necessarily ad hoc combinations of available variables. They arrive at this conclusion by examining the problem of predicting civil war onset. They report that a parsimonious model using a small number of covariates derived from escalation theories of conflict can forecast civil war onset better than alternative specifications based on generic covariates not specifically informed by theory including a \textit{kitchen sink} model with more than 1,000 covariates.

B&S specifically examine three questions: 

1. How does the theoretically-driven escalation model compare in forecast performance to alternative models not informed specifically by civil war onset theories?
2. Does annual, structural information from the PITF instability forecasting model add to the escalation model's monthly and 6-month predictions?
3. How accurate were predictions using the escalation model for the first half of 2016?

To assess the first two questions, B&S use ICEWS data covering all major countries from 2001 to 2015. Two versions of the dataset are used, one at the country-month level, the other aggregated to 6-month half-years. The main outcome variable is civil war onset, measured using Sambanis' civil war dataset. 

Both the first and second questions above rely on comparing their escalation model to various alternative models. The same procedure is used in both cases:

1. Split the training data into training (2001 - 2007) and test (2008 - 2015) sets.
2. Estimate the escalation and other competing models. 
3. Create out-of-sample (OOS) predictions from each model using the test set. 
4. Calculate AUC-ROC measures for each set of OOS predictions.

To examine the first question, B&S compare the test set of the escalation model to four alternative models. The independent variables for the first set of analysis reported in Table 1 in the paper are all derived from the ICEWS event data, using domestic events between actors within a country. The models are:
- Escalation: a set of ten indicators, putatively drawn from a theoretical escalation model.
- Quad: ICEWS quad counts, i.e. material conflict, material cooperation, verbal conflict, verbal cooperation.
- Goldstein: -10 (conflictual) to 10 (cooperative) scores derived from the ICEWS data for interactions between the government on one side and opposition or rebel actors on the other. These are directed, thus making for four total covariates.
- CAMEO: counts for all CAMEO event codes totaling $1,159$ covariates, which are mostly zero for any country in any month. 
- Average: unweighted average of the predictions from the four models briefly described above.


The corresponding results for each question are shown in B&S Tables 1 and 2, which we will examine further below. We accurately replicate their Tables 1 and 2, with very tiny differences. 
The results in Table 1, aside from the core base specification results, include eight additional robustness tests for both the 1-month and 6-month versions. These robustness checks vary either (1) random forecast hyperparameter values, or (2) the year used to split the train/test data, or (3) alternative codings of the civil war onset dependent variable. 

The second question, whether structural variables add to the escalation model, is assessed by comparing the original escalation model to four alternatives that incorporate annual, structural variables that are used in the PITF instability forecasting model:

- Escalation Only: the original basic escalation model with only ICEWS predictors
- With PITF Predictors: a random forest that as predictors has the escalation model indicators but also the PITF annual, structural variables
- Weighted by PITF: escalation model predictions weighted using the PITF instability model predictions
- PITF Split Population: the training data are split into high and low risk portions based on the PITF instability model predictions, two separate escalation random forests are trained on the splits, then re-combined into a single random forest that is used to create the test set predictions
- PITF Only: a random forest model based only on the annual, structural PITF model predictors

The corresponding results are shown in B&S Table 2. 

Finally, B&S used their escalation model to create forecasts for the first half of 2016, and in their third and final analysis, they score the forecasts accuracy using civil war onset data later observed. This is summarized in B&S Table 3. 

# Implementation issues in B&S

While replicating and analyzing B&S's results, we found several issues worthy of further discussion and investigation. These are a) the use of smoothed ROC curves to draw conclusions about which model is best, b) incorrect implementations of the weighted by PITF and PITF analog split-population models, c) inconsistent test sets for the models examined, and d) incorrect scoring of the 2016 forecasts.^[We also note (e) additional concerns arising from the question of how the random forest models were tuned by B&S, especially given the way they are used is unorthodox. We did not further investigate the latter issue as it is rendered somewhat moot by the changes in results after addressing the preceding issues.]

We believe that these research decisions and issues lead B\&S to incorrect conclusions. The escalation model is not the best, and it actually performs worse than the atheoretical, garbage can model with over \(1000\) variables. We discuss these five issues below. We defer a complete analysis that corrects all these issues until later, as there are many possible permutations of a seriatim unfolding.

## Smoothed ROC curves

The most consequential issue that we found is that all AUC-ROC values reported in B&S Tables 1 and 2 are calculated using smoothed ROC curves, not the original, empirical ROC curves. The data in rare events problems like this one restricts the number of true positive rate values and leads to very non-smooth ROC curves; B&S refer to smoothing in the context of their Figure 1 with ROC curves as easing interpretation. But in fact smoothing is used to produce all AUC-ROC values they report. Standard practice has been to use empirical, not smoothed ROC curves, both for visualization and when calculating AUCs. 

Aside from the visual impact of ROC smoothing, the underlying motivation and methods involved in smoothing are much broader and involve parametric estimation of population ROC curves [@henley:1988; @henley:2014]. The main application is in the evaluation of medical tests, e.g. diagnostic imaging, with early methods dating back several decades and ongoing development of both parametric and non-parametric smoothing methods [e.g. @zou:etal:1997; @pulit:2016]; it does not seem to be established which methods should be preferable in a given application [@henley:2014]. It is also not clear whether these methods and rationales are valid in applications like our present one, e.g. where our data is closer to a census of the population and with well-known dependencies like spatial and temporal correlation, rather than a random sample that is approximately i.i.d. 

The next three issues we encountered all concern information in B&S Table 2. 

## Incorrect "Weighted by PITF" implementation

The "Weighted by PITF" model is described as follows in B&S, page 19:

> The [Weighted by PITF model] uses PITF predicted probabilities to weight the results of the escalation model, ensuring that high-risk and low-risk countries that happen to take similar values on ICEWS-based predictors are nonetheless assigned different predicted probabilities in most months.

B&S intend that the escalation model's predictions for the test set are weighted by the PITF model predictions for the test set. B&S, however, actually weights the \textbf{test} set predictions using the PITF model predictions for the \textbf{training} set.^[See `1mo_run_escalation_weighted_PITF.R` line 4, where the PITF predictions are taken from the training data set (`train$pred_prob_plus1`). The next line is a hack extending the shorter `weight` vector with missing values to avoid a R warning when it is multiplied with the longer vector of escalation model test set predictions. Similarly, in the 6-month version of this file.] This appears to be an easily corrected coding error on the part of B&S.


## Incorrect "PITF Split Population" implementation

The "PITF Split Population" model appears to be incorrectly implemented, owing to a coding error. B&S describe it on page 20:

> The final approach is a random forest analog to split-population modeling.
first compute the average PITF predicted probability for each country across all years in our training set. We define those that fall in the bottom quartile as "low risk" and the rest as "high risk." We then run our escalation model on the high-risk and low-risk subsets separately, combining the results into a single random forest (column 4).

The intention of B&S appears to have been to run two separate random forest models, one each on the low- and high-risk training data splits. The replication code does indeed run two separate random forecasts, but they both utilize the *exact same training data*, which consists of the full training data from all other models. The models are also identical otherwise, i.e. they use the same *x* variables and the same random forest hyper-parameter settings. The *only* difference in the models as they are implemented in the B&S replication code is due to the non-deterministic nature of the random forest model itself. If we ran both with the same random seed, they would be identical in every respect, producing identical predictions.^[Disentangling this coding error is not straightforward as it occurs over several R scripts and requires (or at least is easier to verify by) running partway through the actual replication until the objects holding the training data for the models are instantiated and can be examined. We have documented details at https://github.com/rickmorgan2/Blair-Sambanis-replication/issues/5.] 

The implementation error aside, this split-population analog model is quite odd and does not replicate the idea behind split-population modeling. Although the two RFs are trained on separate data (in our updated, fixed replication), the process of combining them actually just creates a new, larger RF using both component model's underlying decision trees. Thus, while all RF models throughout (except for one of the robustness checks) are trained with 100,000 decision trees (`ntree`), the new RF model after combination does have 200,000 decision trees. Furthermore, the PITF model predictions do not impact how the combined RF model predicts at all, not even through a binary low-/high-risk split. The split-population PITF RF model is practically speaking just another escalation model trained with N=200,000 instead of N=100,000 trees and an extra odd randomization step added to the already existing RF randomization facilities (row and column sampling for each decision tree). This does not adequately implement their research strategy.

## Inconsistent test set N for the models in Table 2

Further, the AUC-ROC values reported in the original B&S Table 2 are calculated based on slightly different numbers of underlying test set cases (see Table \ref{tab:table2-N}). ROC calculations for a set of predictions can only be done on the set of cases for which both non-missing predictions and non-missing outcomes are available. Those sets differ across models (columns) for each row in B&S Table 2. 

Thus a difference in AUC-ROC values for two models could be because they were calculated on different sets of underlying cases, not because the models are systemically performing at a different level. In other words, the results for different models in B&S Table 2 are not comparable to one another, and any conclusions drawn from such comparison are potentially incorrect. 

## Incorrect scoring of the 2016 forecasts

B&S present a confusion matrix to score their 2016-H1 forecasts in Table 4. Although the forecasts are for the probability of civil war onset, in the replication code, they are actually scored using the much more commonly used incidence of civil war, i.e. including ongoing civil wars as "1"'s. 

The relevant variables in the data are "incidence_civil_ns" and "incidence_civil_ns_plus1", which appears to be a 1-period lead version of the DV that is used in the actual prediction models. The incidence DV contains both 0/1 and missing values. By examining the pattern of missing values, it seems clear that this was originally an incidence variable indicating whether a country was at civil war in a given year or not, and which was converted to an onset version so that onsets retain the value of 1 but continuing civil war years are coded as missing. This reflects common practice in how these are coded. 

By examining the code used to generated Table 4, we were able to confirm that the onset forecasts are assessed using incidence, not onset. In the file `6mo_make_confusion_matrix.do` on line 52, missing values in "incidence_civil_ns" are recoded to 1, thus reverting the onset coding of this variable to incidence. 

# Results of the updated analysis

We now turn to an examination of our analysis that addresses and fixes the issues discussed above.^[The code for all of our analysis undertaken for this effort may be found at https://github.com/rickmorgan2/Blair-Sambanis-replication.] The main results of the original analysis consist of the comparison of the Escalation to other ICEWS models (our Table \ref{tab:table1-nosmooth}, B&S Table 1), and a comparison of the Escalation model to models that add structural variables/information (our Table \ref{tab:table2-fixed}, B&S Table 2). We will review the substantive implications of our updated analysis further below, but the bottom line is that these changes turn B&S's conclusions on their heads.

## Smoothed ROC curves and AUC calculations

A reference to smoothing is made in a single sentence in the paper (p. 12):

> Figure 1 displays the corresponding ROC curves, smoothed for ease of interpretation.

This implies that the ROC curves were only smoothed in the referenced Figure 1, but actually all AUC-ROC calculations throughout the replication code use an option to smooth the ROC curves prior to AUC calculation. ROC curves are constructed from the false positive and true positive rates as one moves through a set of ranked predictions, and as a result appear step-like. Figure 1 shows our replication of both the smoothed ROC curves B&S report, and the actual ROC curves on the right. The standard method is to compute the area under the curve AUC statistic on the original, non-smooth ROC curves that are shown on the right. 

![Replication of B&S Figure 1 with both smooth and non-smoothed ROC curves.](figures/figure1-replicated.png)
The specific predictions also include groups of cases with identical predicted probabilities, which accounts for the unusual diagonal lines seen in the panels on the right. In any case, with a sparse outcome like civil war onsets, the true positive rate on the *y*-axis only changes when the prediction for an observed positive case is reached. For these ROC curves, and for that matter in the fundamental train/test split used for 12 of the 18 rows/models in B&S Table 1, there are only 11 civil war onset cases in the test set. Thus, the ROC curves here are very step-like, with only 12 (11 positive cases plus 1 for TPR = 0) distinct *y* coordinates.  Notice also that the smoothing averages the left-most almost straight line with the right-most almost straight line in a monotonic way.

## The effect of using smoothed ROC curves

What impact did the ROC smoothing have overall on the performance of the Escalation model relative to other ICEWS models (our Table \ref{tab:table1-nosmooth}, B&S Table 1) and structural extensions (our Table \ref{tab:table2-fixed}, B&S Table 2)?  Figure \ref{fig:benefit-plot} shows the changes in AUC-ROC values had we used smoothed ROC curves to calculate the AUC-ROC values. Each of the gray points corresponds to the change in AUC-ROC values for one of the models in the cells in Tables \ref{tab:table1-nosmooth} and \ref{tab:table2-fixed}. The red crosses mark the average effect of smoothing on AUC-ROC. For all alternative models, from Quad to "PITF Only", smoothing sometimes hurts and sometimes benefits, but the overall impact is negligible in average. The Escalation model _always_ benefits from smoothing, with an average improvement on the order of 0.05. This is sufficient to push the Escalation model ahead of the alternative models, and accounts for the result B&S find, namely that the escalation model is generally superior. 

```{r benefit-plot, fig.height=3.5, fig.width=5.5, out.width = ".8\\linewidth", fig.align = "center", fig.cap = "Gain from using smoothed ROC to calculate AUC, for each model reported in Tables 3 and 4 (B\\&S Tables 1 and 2)"}
table1_benefit <- read_csv("data/table1-smooth-benefit.csv") 
table2_benefit <- read_csv("data/table2-smooth-benefit.csv") 

benefit <- bind_rows(
  table1_benefit %>%
    rename(setting = Model) %>%
    pivot_longer(-c(horizon, setting), names_to = "model", values_to = "diff"),
  table2_benefit %>%
    rename(setting = Model, Escalation = `Escalation Only`) %>%
    pivot_longer(-c(horizon, setting), names_to = "model", values_to = "diff")
)

# mean benefit by model
means <- benefit %>%
  group_by(model) %>%
  summarize(avg_benefit = mean(diff))

ann_text <- tibble(horizon = "1 month", diff = 0.07, model = 8.6, label = "Mean")

model_levels <- c("Escalation", "Quad", "Goldstein", "CAMEO", "Average", 
                  "With PITF Predictors", "Weighted by PITF", "PITF Split Population",
                  "PITF Only")
benefit %>%
  mutate(model = factor(model, levels = rev(model_levels))) %>%
  ggplot(aes(y = model)) + 
  #facet_wrap(~ horizon, scales = "free_x") +
  geom_point(aes(x = diff), alpha = 0.5, size = 2, color = "gray50") +
  geom_point(data = means, aes(x = avg_benefit), shape = "|", color = "black", size = 6) +
  labs(x = "AUC(smoothed ROC) - AUC(ROC)",
       y = "") +
  theme_bw() +
  geom_text(data = ann_text, aes(x = diff, label = label), col = "black") +
  geom_vline(xintercept = 0, linetype = 3) +
  theme(axis.text.y = element_text(size = 12))
```

In sum, it is not only the case that using smoothed ROC curves alters the results: the use of smoothed ROC curves to calculate AUC-ROC values benefits \textbf{only} the escalation model. It does so consistently and by a considerable margin. All eight alternative models reported in B&S Tables 1 and 2, on average, do not gain when using smoothed ROC curves to calculate AUC. 

## Is the Escalation model superior to the alternative ICEWS models?

Table \ref{tab:table1-nosmooth} (B&S Table 1) shows the comparison of the Escalation model to other alternative models based on ICEWS event data indicators. The AUC-ROC values are based on original, non-smoothed ROC curves. 

Both the Average and CAMEO models outperform the Escalation model in almost all instances. The Goldstein model generally outperforms the Escalation model in the 6-month version. The Quad model appears to be roughly on par with the Escalation model. Thus, the original B&S conclusion that the escalation model is superior to the alternative models is entirely conditional on the non-standard use of smoothed ROC curves and overturns when using traditional AUC-ROC calculations. 

```{r table1-nosmooth}
table1_nosmooth <- read_csv("data/table1-nosmooth.csv")

table1_nosmooth %>%
  select(-horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Comparison of the escalation model to alternative ICEWS model using test set AUC-ROC, \\textit{without} smoothed ROC curves (Replication of B\\&S Table 1)",
    label = "table1-nosmooth") %>% 
  pack_rows(index = c("One-month forecasts" = 9, "Six-month forecasts" = 9))
```

The key difference between Table \ref{tab:table1-nosmooth} and B&S Table 1 is whether the underlying ROC curves were original or smoothed; the test set N and coding issues did not affect this set of results.^[We should note that in our replication we consciously decided to not set RNG seeds, even though the random forest models are non-deterministic and vary slightly from run to run. As we changed the replication code to allow running in parallel, we cannot exactly reproduce the B&S results even with the same RNG seed. More importantly, the substantive interpretation of results should not depend on a specific RNG seed. We found that the escalation model's AUC-ROC values generally fluctuate no more than 0.01 (see https://github.com/rickmorgan2/Blair-Sambanis-replication/blob/master/rep_nosmooth/variance.md), and thus are confident that the patterns we see are robust to the initial RNG state.] B&S's original results and interpretation regarding the superiority of the B&S model over alternative ICEWS models, including the 1,000+ covariate CAMEO model, are thus entirely conditional on the use of smoothed AUC-ROC. Even the Quad model is generally as good as the escalation model in all implementations and is frequently quite a bit better.

## Using Inconsistent Ns

The top portion of Table \ref{tab:table2-N} shows the number of valid test set predictions that can be scored for each model in the original B&S Table 2. For the 1-month data version, the number of predictions differs by up to 500 cases, and in the 6-month data version by around 100 cases. These numbers appear small enough to be not important. However, the already small number of positive cases also is affected substantially:  "With PITF Predictors" and "PITF Only" models lose two or one (respectively) of 11 positive cases in the 1- and 6-month data versions. 

```{r}
table2_full <- read_csv("data/table2-for-appendix.csv")

table2_full %>%
  select(horizon, column, N_test, cases) %>%
  pivot_wider(names_from = column, values_from = N_test) %>%
  arrange(horizon, desc(cases)) %>%
  rename(Cases = cases, Horizon = horizon) %>%
  select(Cases, Horizon, everything()) %>%
  arrange(desc(Cases), Horizon) %>%
  knitr::kable(
    "latex", booktabs = TRUE, digits = 0,
    caption = "Number of valid test predictions for the escalation and structural alterantives models (B\\&S Table 2)",
    label = "table2-N") %>%
  column_spec(3:7, width = "2cm") %>%
  collapse_rows(1:2, row_group_label_position = "stack") 
```

Generally, these comparisons require the same cases.  Therefore, we use predictions for a common joint subset of non-missing predictions. The resulting numbers of cases for each model are shown in the bottom portion of the table. Since the sets of incomplete cases in the original version above do not entirely overlap themselves, the common subset for all models is slightly smaller than the smallest N in the top portion of the table.

## Do structural variables add to the Escalation model?

Table \ref{tab:table2-fixed} shows our replication of B&S Table 2 with (1) regular, not smoothed, AUC-ROC, (2) fixed "Weighted by PITF" and "PITF Split-Population" models, and (3) AUC-ROC values computed on the common, joint subset of tests cases for which all models have non-missing predictions. Table \ref{tab:table2-full} in the appendix shows AUC-ROC values for both smoothed and non-smoothed versions, and both the original, model-varying test cases sets and our common joint subset. 

```{r table2-nosmooth}
table2_nosmooth <- read_csv("data/table2-nosmooth.csv")

table2_nosmooth %>%
  dplyr::select(-Model, -horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Comparison of escalation model to structural extensions, using test set AUC-ROC (Replication of B\\&S Table 2)",
    label = "table2-fixed") %>%
  column_spec(2:6, width = "2cm") %>%
  pack_rows(index = c("One-month forecasts" = 1, "Six-month forecasts" = 1)) %>%
  footnote("Differences from the original B\\&S Table 2: (1) AUC-ROC values are computed on the common subset of cases, meaning that N is equal in each row; (2) AUC-ROC values are computed using original, non-smoothed ROC curves.", footnote_as_chunk = TRUE, threeparttable = TRUE)
```

B&S interpret their results as follows, on page 20 and we comment on the conclusions seratim:^[We list the "Overall, ..." interpretation out of order, last, for clarity.]

1. B&S: ``Of the approaches we test, the split-population analog is most promising \ldots ''

The PITF split-population analog still performs well, but the simple Escalation + PITF predictors model arguable performs better still. 

2. B&S: ``Adding PITF predictors improves the performance of the escalation model over six-month windows but diminishes it over one-month windows \ldots ''

Adding PITF predictors actually improves performance in both cases; the "With PITF Predictions" model strictly dominates the "Escalation Only" model, and by quite a maring in the 6-month data version.  

3. B&S: ``The weighted model performs very poorly regardless \ldots ''

The weighted model performs roughly on par with the Escalation Only model. One finding that remains is that the "PITF Only" model is outperformed by the "Escalation Only" model. As the former only uses annual inputs, but the data at hand are the 1-month or 6-months level, this is neither surprising, nor noteworthy.

4. B&S: ``Overall, our results suggest that while measures of structural risk may improve predictive performance, the value they add is marginal and inconsistent. [...] Incorporating PITF thus significantly reduces or only slightly improves the performance of the escalation model, regardless of the approach we take. \ldots ''

The most straightforward method of incorporating the annual structural PITF variables--adding them to the predictors of the Escalation RF model--strictly outperforms the Escalation Only model. Note that the two other combination models considered are both non-standard and that the "PITF Split Population" model does not incorporate structural information at all. We thus conclude that adding structural variables improves predictive performance.

## How accurate were the 2016 forecasts?


B&S report confusion matrices for their forecasts for the first half of 2016 (2016-H1) in Table 4. To create the confusion matrices, B&S treat the 30 highest ranked predictions as positive predictions ("1s") and the rest as negative predictions ("0s"). We replicate their Table 4 in the top portion of \ref{tab:table4}. There are two confusion matrices for slightly different codings of outcomes in 2016-H1, under the corresponding "Assuming Persistence" and "Assuming Change" headings. We should note that the "Assuming Persistence" corresponds to the values in the replication data; the small variations for the "Assuming Change" version are hand-coded in the replication code file that generates the confusion matrices and appear to be subjective assessments of B&S. 

We can see that the original table presents 15 or 16 positive cases for 2016-H1, depending on the dependent variable coding variation. This corresponds to a positive rate of around 9.5% for the first half of 2016 data. In contrast, the corresponding 6-month version of the data from 2001 to 2015, with 30 half-years, has in total 20 civil war onset events, for a much lower positive rate of around 0.5%. The positive event rate in the confusion matrices far exceeds the rate of observed civil war onsets in both the training and test data. This suggests that the forecasts were erroneously assessed using civil war incidence, not onset. By examining the replication code we were able to verify that the forecasts were scored using civil war incidence, i.e., including ongoing civil wars, rather than civil war onset years only. 

```{r, message=FALSE}
tab4o <- read_csv("data/table4-original.csv")
tab4f <- read_csv("data/table4-fixed.csv")

tab4o <- tab4o %>%
  rename(header = type) %>%
  mutate(source = "Original, scored with civil war incidence")
tab4f <- tab4f %>%
  rename(Predicted0 = `0`, Predicted1 = `1`) %>%
  mutate(source = "Fixed, scored with civil war onset")

tab4 <- bind_rows(tab4o, tab4f) %>%
  select(source, header, everything())

tab4 %>%
  knitr::kable(
    caption = "Replication of B\\&S Table 4: 2016 Confusion Matrices for Six-month Escalation Model.",
    label = "table4",
    booktabs = TRUE
  ) %>%
  kableExtra::collapse_rows(1:2, row_group_label_position = "stack")
```

The correct confusion matrices when using observed onset (or the lack of it) are shown in the second part of Table \ref{tab:table4}. In the default "Assuming Persistence" coding, there are no civil war onsets in the data for 2016-H1. Thus, the recall values is undefined, while the precision is 0/30 = 0, compared to reported recall and precision values of 13/15 = 0.87 and 13/30 = 0.43. The alternative coding ("Assuming Change") produces two civil war onsets. Recall is 1.0 compared to 14/16 = 0.88 before, and precision is 2/30 = 0.07 instead of 14/30 = 0.47. 

Another, minor issue or rather coding error, is related to using a lead version of the DV. With the lead version of the DV, "incidence_civil_ns_plus1", which is what the models are predicting, the predicted value for 2016-H1 indicates the risk of civil war onset in 2016-H2. In the Table 4 script referenced above, the 2016-H1 predictions (for 2016-H2) are assessed using the raw DV, "incidence_civil_ns", not the lead version. Essentially, the forecasts for 2016-H2 are assessed using observed outcomes for 2016-H1. In this case, it doesn't make a difference since both the raw DV and lead version for 2016-H1 do not have any positive values. 

# Conclusion

The B&S advocate the use theory to guide prediction. But theory as used by B&S is an ambiguous and undefined concept.  It is not a procedure.  They actually create a model with ten right hand side variables that are supposed to capture a complicated repression-dissent dynamic. There is wide-ranging literature on this dynamic that could justify many specifications.  As such, their baseline comparison is an unfortunate standard bearer for strong theory.

Moreover, they misunderstand the use of ICEWS event data in current research. They claim that most applications to date have focused on the quad categories, but this ignores a wide swath of literature [@steinert-threlkeld:2017; and @metternich:etal:2013] that uses a specific action---such as protest---defined in the CAMEO ontology. In their article on conflict in Thailand, Metternich et alia hand-coded, for example, every actor in Thailand and focused on an analysis of how those interact.

We encountered several issues in the code underlying the B&S analysis. The problems we encountered are not subjective modeling choices. When we fix these issues and perform an updated analysis, the B&S conclusions are all essentially overturned. In other words, B&S findings are based on a faulty analysis, and invalid. 

Using the same analysis B&S intend to use, we, in fact, find that:
- the theory-driven escalation model is outperformed both by the low-effort 1,160 predictor all-CAMEO model. The Average ensemble model
- structural variables substantially improve the escalation model's performance when added to the pool of predictors on which the underlying random forest model is based.

Blair & Sambanis have focused attention on comparing forecasting models and forecasts in the civil war domain.  As social science becomes more adept at predictive analysis, this will doubtless be of increasing importance.  However, these comparisons must be made carefully to ensure those correct inferences are drawn.  We continue to think that theory is overrated [@ward:2016], and that machine learning and big data will allow us to learn new things, but it will be interesting to see how the evidence for these claims are adjudicated with additional usage and careful evaluation.


# References

<div id="refs"></div>

\newpage

# (APPENDIX) Appendix {-}

# Appendix

\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}

\renewcommand{\thetable}{A\arabic{table}} 
\setcounter{table}{0}

## Additional replication tables

Table \ref{tab:table1-smooth} is our replication of B&S Table 1 with smoothed AUC-ROC. The results differ slightly from the original B&S Table 1, typically by no more than 0.01, due to the non-deterministic nature of the RF models. It is the case that B&S set the RNG seed in their replication code, which should theoretically allow exact reproduction, but (1) there was a change in more recent versions of R that affected the RNG seeding process, and (2) we refactored the replication script to allow one to run the models in parallel. In any case, the interpretation of results should not be sensitive to random variation, i.e. it should not depend on using a specific RNG seed. On the basis of these results, B&S conclude that the escalation model is generally superior to the alternatives, and we can replicate that interpretation when using smoothed ROC curves. 

```{r table1-smooth}
table1_smooth <- read_csv("data/table1-smooth.csv")

table1_smooth %>%
  select(-horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Replication of B\\&S Table 1 with smoothed ROC curves; test set AUC-ROC for various models",
    label = "table1-smooth") %>% 
  pack_rows(index = c("One-month forecasts" = 9, "Six-month forecasts" = 9))
```

```{r}
table2_full <- read_csv("data/table2-for-appendix.csv")
  
table2_full %>%
  pivot_longer(auc_roc:auc_roc_smoothed, names_to = "Smoothed ROC",
               values_to = "auc_roc") %>%
  mutate(`Smoothed ROC` = case_when(
    `Smoothed ROC`=="auc_roc" ~ "No",
    TRUE ~ "Yes")
  ) %>%
  select(horizon, column, cases, `Smoothed ROC`, auc_roc) %>%
  pivot_wider(names_from = column, values_from = auc_roc) %>%
  rename(Cases = cases, Horizon = horizon) %>%
  select(Cases, Horizon, `Smoothed ROC`, everything()) %>%
  arrange(desc(Cases), Horizon, desc(`Smoothed ROC`)) %>%
  knitr::kable(
    "latex", booktabs = TRUE, digits = 2,
               caption = "Replication of B\\&S Table 2 with smoothed/original ROC and with original varying N cases or adjusting for common case set with constant N",
    label = "table2-full") %>%
  column_spec(3, width = "1.5cm") %>%
  column_spec(4:8, width = "2cm") %>%
  collapse_rows(1:3, row_group_label_position = "stack")

```

```{r smooth-benefit-tables}
table1_benefit <- read_csv("data/table1-smooth-benefit.csv") 
table1_benefit %>%
  select(-horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Smoothing advantage for B\\&S Table 1: the gain in AUC-ROC when calculated using smoothed ROC curves",
    label = "table1-benefit") %>% 
  pack_rows(index = c("One-month forecasts" = 9, "Six-month forecasts" = 9))

table2_benefit <- read_csv("data/table2-smooth-benefit.csv") 
table2_benefit %>%
  select(-horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Smoothing advantage for B\\&S Table 2: the gain in AUC-ROC when calculated using smoothed ROC curves",
    label = "table2-benefit") %>% 
  column_spec(2:6, width = "2cm") %>%
  pack_rows(index = c("One-month forecasts" = 1, "Six-month forecasts" = 1))
```

## Model to model comparison plots

Figures \ref{fig:table1-plot} and \ref{fig:table2-plot} replicate the information in Tables \ref{tab:table1-nosmooth} and \ref{tab:table2-fixed} in a way makes the comparison of the escalation model to the alternative models easier. Each facet shows the escalation AUC-ROC for all model settings (the rows in Table \ref{fig:table1-plot} and base specification for all models in Table \ref{tab:table2-fixed}) on the left, and the AUC-ROC for an alternative model on the right, with a connecting line. If the lines slope up to the right, the alternative model is better. 

```{r table1-plot, fig.height=5, fig.width=8, out.width = ".9\\linewidth", fig.align = "center", fig.cap = "Escalation to alternative comparisons for the ICEWS models (B\\&S Table 1)"}
table1_nosmooth <- read_csv("data/table1-nosmooth.csv")

icews_lvls <- c("Escalation", "Quad", "Goldstein", "CAMEO", "Average")

df <- table1_nosmooth %>%
  rename(Spec = Model) %>%
  pivot_longer(Escalation:Average, names_to = "Model", values_to = "auc_roc") %>%
  mutate(Model = factor(Model, levels = icews_lvls)) %>%
  nest(data = c(Spec, auc_roc))

pairs <- full_join(
  df %>% filter(Model!="Escalation"),
  df %>% filter(Model=="Escalation") %>% rename(data2 = data) %>% select(horizon, data2),
  by = c("horizon")
) %>%
  pivot_longer(data:data2, names_to = "xname", values_to = "data") %>%
  mutate(xname = ifelse(xname=="data", as.character(Model), "Escalation"),
         xname = factor(xname, levels = icews_lvls))

pairs %>%
  unnest(data) %>%
  ggplot(aes(x = xname, y = auc_roc)) +
  facet_grid(horizon ~ Model, scales = "free_x") +
  geom_point() +
  geom_line(aes(group = Spec)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank()) +
  labs(x = "", y = "AUC-ROC (empirical)")
```

```{r table2-plot, fig.height=5, fig.width=8, out.width = ".9\\linewidth", fig.align = "center", fig.cap = "Escalation to alternative comparisons for the structural models (B\\&S Table 2)"}
table2_nosmooth <- read_csv("data/table2-nosmooth.csv")

model_lvls <- colnames(table2_nosmooth)[-c(1:2)]

df <- table2_nosmooth %>%
  rename(Spec = Model) %>%
  pivot_longer(`Escalation Only`:`PITF Only`, names_to = "Model", values_to = "auc_roc") %>%
  mutate(Model = factor(Model, levels = model_lvls)) %>%
  nest(data = c(Spec, auc_roc))

pairs <- full_join(
  df %>% filter(Model!="Escalation Only"),
  df %>% filter(Model=="Escalation Only") %>% rename(data2 = data) %>% select(horizon, data2),
  by = c("horizon")
) %>%
  pivot_longer(data:data2, names_to = "xname", values_to = "data") %>%
  mutate(xname = ifelse(xname=="data", as.character(Model), "Escalation Only"),
         xname = factor(xname, levels = model_lvls))

pairs %>%
  unnest(data) %>%
  ggplot(aes(x = xname, y = auc_roc)) +
  facet_grid(horizon ~ Model, scales = "free_x") +
  geom_point() +
  geom_line(aes(group = Spec)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(x = "", y = "AUC-ROC (empirical)")
```


## Random forest hyper-parameter selection

What initially sparked our interest in the paper was the unusual choice of hyperparameter settings for the random forest models estimated. Table \ref{tab:hp} shows the default values used by the implementation of random forest that B&S use (from the **randomForest** R package), in contrast to the basic settings used by B&S for most the models reported in the paper. 

As the outcome is a binary indicator of civil war onset, one would typically use a classification random forest that predicts 0 or 1 labels directly. The implementation of random forests that B&S use ([@liaw:wiener:2002]) is based on the original @breiman:2001 implementation and calculates predictive probabilities by averaging over the "0" or "1" votes from all constituent decision trees. The conventional wisdom regarding the number of trees in a random forest is that it needs to be large enough to stabilize performance, but without any additional gain or harm in accuracy beyond a certain number. From the other default settings, which are generally not uninformed choices, one can see that the basic logic is to grow a forest with a relatively small number of trees, but where each tree is fairly extensive, and operates on a large bootstrapped sample of the original training data. These are of course only heuristics and it is usual to attempt to find better hyper-parameter methods through some form of tuning. 

\begin{table}
\caption{\label{tab:hp} Random forest (\texttt{randomForest()}) default versus B\&S hyperparameters}
\begin{tabular}{l>{\raggedright\arraybackslash}p{2in}ll}
\toprule
Hyperparameter & Default heuristic & Default values (Escalation) & B\&S value \\
\midrule
type & & classification & regression \\
ntree & & 500 & 100,000 or 1e6 \\
mtry & \texttt{floor(sqrt(ncol(x)))} & 3 & 3 \\
replace & & true & false \\
sampsize & \texttt{nrow(x)} if replace, else \texttt{ceiling(.632*nrow(x))} & 11,869 & 100 or
500 \\
nodesize & 1 for classification & 1 & 1 \\
maxnodes & & null & 5 or 10 \\
\bottomrule
\end{tabular}
\end{table}

B&S in contrast fit very large forests with 100,000 trees in the basic model form, but where each tree only operates on a very small sub-sample (N=100 or 500), drawn without replacement, of the available training data. This approach only works due to the choice to use regression, not classification, trees. Trying to use classification trees with the other parameter settings not work at all because it is almost guaranteed that a sample of 100 from the ~11,000 training data rows with 9 positive cases will only include 0 (negative) outcomes in the sample. As it is, using regression with a 0 or 1 outcome produces warnings when estimating the models: 

```
Warning message:
In randomForest.default(y = as.integer(train_df$incidence_civil_ns_plus1 ==  :
  The response has five or fewer unique values.  Are you sure you want to do regression?
```

As it turns out, using regression random forests for this kind of binary classification problem in order to obtain probability estimates matches the probability random forest approach suggested and positively evaluated in @malley:etal:2012, and which is used in another prominent R implementation of random forests.^[The **ranger** package.] It is not clear whether this is intentional, as the Malley paper is not cited in B&S. 

In any case, B&S's random forest approach appears to work really well. We tried to construct classification random forests tuned via cross-validation on the training data set partition, i.e. without touching the test data, but were unable to develop models that consistently match the B&S random forest method in both cross-validated out-of-sample training predictions and test set predictions. 

Given that they are relatively unorthodox, yet appear to work very well, we wonder how the hyper-parameter values were determined. Two specific concerns are that this was not done with an eye towards test set accuracy, which would invalidate the independence of the out-of-sample test set, and whether the specific hyper-parameter values are optimized for only one model, or were optimized and found to work well for all models. There is no discussion of the random forest tuning strategy or how the specific hyper-parameter methods were determined in the paper. 

Given the dramatic changes in results as a result of the preceding issues, we did not further investigate these potential concerns.

