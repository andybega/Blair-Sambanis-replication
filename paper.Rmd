---
title: "Reassessing the Role of Theory and Machine Learning in Forecasting Civil Conflict"
author: 
 - Andreas Beger^[Predictive Heuristics, adbeger@gmail.com.]
 - Richard K. Morgan^[Varieties of Democracy Institute, University of Gothenburg, rick.morgan2@gmail.com.]
 - Michael D. Ward^[Predictive Heuristics, Duke University, University of Washington, michael.don.ward@gmail.com. Corresponding author.]
date: "`r format(Sys.time(), '%d %B %Y')`"
thanks: John Ahlquist, Cassy L. Dorff, and Shahryar Minhas both provided helpful feedback on this project. We are especially grateful to Paul Huth for his comments and guidance. All the code and several additional analyses can be found at our replication archive at https://github.com/andybega/Blair-Sambanis-replication.
abstract: We examine the research protocols in Blair and Sambanis (nd).  We find that there are several important mistakes and research decisions that determine their results. Fixing these mistakes results in a reversal of their claim that theory based models of escalation are better at predicting onsets of civil war than other kinds of models.  Their model is s outperformed by several of the ad hoc, putatively non-theoretical models they devise and examine.  
output: 
  bookdown::pdf_document2:
    keep_tex: true
    keep_md: true
    toc: false
#header-includes:
#  - \usepackage{setspace}\doublespacing
bibliography: "./references.bib"
nocite: |
  @cederman:weidmann:2017
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE)
# options(tinytex.verbose = TRUE)

library(bookdown)
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(kableExtra)
library(stringr)
```
# Introduction

Blair and Sambanis [-@blair:sambanis:2020, hereafter B\&S] argue that theory is essential for creating models that have high accuracy in forecasting civil war onset. Indeed they assert that with such theory forecasting is more accurate than has previously been possible. We re-examine the empirical basis for the claims made in support of it. We find that these claims to be unsupported and evidence presented for them to be incorrect. Their theory-based escalation model does not do better than the alternatives that they examine. It does worse. The reason for this is that they have made several mistakes in their research procedure. Further, the performance results they report are based on smoothed performance curves, not the empirical, unsmoothed curves. This provides misleading results. In addition, two of the structural alternatives to their basic escalation model were incorrectly implemented. We also found that the scoring of their forecasts for the first half of 2016 was incorrectly performed using civil war incidence, not onset. In what follows, we show the impact of these mistakes on the conclusions. 

B\&S claim (page 3) to show that a model informed by procedural theories of escalation and de-escalation can predict the onset of civil wars "remarkably accurately". Indeed, B&S argue that this theoretical model outperforms four other "more mechanical" alternatives.They also claim that the integration of structure with process is better than alternatives over short forecasting windows. Third, they preregistered the list of thirty countries that have the highest risk of civil war onset. They claim that such prospective predictions are rare in the literature when, in fact, they have been routine for many years with several prominent projects.^[PITF [e.g. @goldstone:etal:2010; @beger:ward:2017], the W-ICEWS project [@ward:etal:2013], VIEWS [@hegre:etal:2019], and others, e.g. @beger:etal:2020.] B\&S claim to be unique in assessing these forecasts. A qualitative analysis of their predictions allows them to conclude that their model is robust. We will return to their analysis later, after correcting the procedural mistakes we found in their research process.

Before proceeding, we quote B&S (page 24):

> Our theoretically driven model generates accurate forecasts, with base specification AUCs of 0.82 and 0.85 over one- and six-month windows, respectively, and AUCs as high as 0.92 in other specifications. Our model also consistently and sometimes dramatically outperforms the alternatives we test. [...]  Cederman and Weidmann (2017, 476) argue that "the hope that big data will somehow yield valid forecasts through theory-free 'brute force' is misplaced in the area of political violence." Our results lend some credence to this claim.

# Summary of Blair and Sambanis (2020)

B&S' analysis is based on the use of non-linear, non-deterministic machine learning models, and specifically random forests, one of which has a specification they argue is theory-based.  Several others with more generic sets of covariates are also developed and compared. Notably, the analysis is at the country-month or country-(6 month) levels and relies in large part on indicators derived from the ICEWS event data. In short, they uphold their assumption that theory-guided empirical research produces better conflict predictions than machine learning inspired efforts that are necessarily ad hoc combinations of available variables. They arrive at this conclusion by examining the problem of predicting civil war onset. They report that a parsimonious model using a small number of covariates derived from escalation theories of conflict can forecast civil war onset better than alternative specifications based on generic covariates not specifically informed by theory, including a \textit{kitchen sink} model with more than 1,000 covariates.

B&S specifically examine three questions: 

1. How does the theoretically-driven escalation model compare in forecast performance to alternative models not informed specifically by civil war onset theories?
2. Does annual, structural information from the PITF instability forecasting model add to the escalation model's monthly and 6-month predictions?
3. How accurate were predictions using the escalation model for the first half of 2016?

To assess the first two questions, B&S use ICEWS data covering all major countries from 2001 to 2015. Two versions of the dataset are used, one at the country-month level, the other aggregated to 6-month half-years. The main outcome variable is civil war onset, measured using Sambanis' civil war dataset. 

Both the first and second questions above rely on comparing their escalation model to various alternative models. The same procedure is used in both cases:

1. Split the training data into training (2001 - 2007) and test (2008 - 2015) data.
2. Estimate the escalation and other competing models with the training data. 
3. Create out-of-sample (OOS) predictions from each model using the test set. 
4. Calculate AUC-ROC measures for each set of OOS predictions.

To examine the first question, B&S compare the test set of the escalation model to four alternative models. The independent variables for the first set of analysis reported in Table 1 in the paper are all derived from the ICEWS event data, using domestic events between actors within a country. The models are:

- Escalation: a set of ten indicators, drawn from a theoretical escalation model, for interactions between the government on one side and opposition or rebel actors on the other. 
- Quad: ICEWS quad counts, i.e. material conflict, material cooperation, verbal conflict, verbal cooperation, for interactions between the government and opposition or rebels. These are directed, thus making for four directed dyads, which with four quad categories make sixteen covariates. 
- Goldstein: -10 (conflictual) to 10 (cooperative) scores derived from the ICEWS data for the same four directed dyads, for a total of four covariates. 
- CAMEO: counts for all CAMEO event codes over the four actor dyads, totaling $1,160$ covariates, which are mostly zero for any country in any month. 
- Average: unweighted average of the predictions from the four models briefly described above.

The corresponding results for each question are shown in B&S Tables 1 and 2, which we examine further below. We accurately replicate their Tables 1 and 2. The results in Table 1, aside from the core base specification results, include eight additional robustness tests for both the 1-month and 6-month versions. These robustness checks vary either (1) random forecast hyperparameter values or (2) the year used to split the train/test data, or (3) alternative codings of the civil war onset dependent variable. 

The second question, whether structural variables add to the escalation model, is assessed by comparing the original escalation model to four alternatives that incorporate annual, structural variables that are used in the PITF instability forecasting model:

- Escalation Only: the original basic escalation model with only ICEWS predictors.
- With PITF Predictors: a random forest that also adds the PITF annual, structural variables.
- Weighted by PITF: escalation model predictions weighted using the PITF instability model predictions.
- PITF Split Population: the training data are split into high and low risk portions based on the PITF instability model predictions, two separate escalation random forests are trained on the splits, then re-combined into a single random forest that is used to create the test set predictions.
- PITF Only: a random forest model based only on the annual, structural PITF model predictors.

The corresponding results are shown in B&S Table 2. 

Finally, B&S used their escalation model to create forecasts for the first half of 2016. In their third analysis, they score the forecasts accuracy using subsequent civil war onset data. This is summarized in B&S Table 3. 

# Implementation Issues in B&S

While replicating and analyzing B&S's results, we found several issues worthy of further discussion and investigation. These are a) the use of smoothed ROC curves to draw conclusions about which model is best, b) incorrect implementations of the weighted by PITF and PITF analog split-population models, c) inconsistent test sets for the models examined, and d) incorrect scoring of the 2016 forecasts.^[We also note there are additional concerns arising from the question of how the random forest models were tuned by B&S, especially given the way they are used is unorthodox. We did not further investigate the latter issue as it is rendered somewhat moot by the changes in results after addressing the preceding issues.]

We believe that these research decisions and issues lead B\&S to incorrect conclusions. The escalation model is not the best; it actually performs worse than the atheoretical, ktchen sink model with over \(1000\) variables. We discuss these five issues below. We defer a complete analysis that corrects all these issues until later, as there are many possible permutations of a seriatim unfolding.

## Smoothed ROC Curves

The most consequential issue that we found is that all AUC-ROC values reported in B&S Tables 1 and 2 are calculated using smoothed ROC curves, not the original, empirical ROC curves. The data in rare events problems like this one restrict the number of true positive rate values and lead to non-continuous ROC curves; B&S refer to smoothing only in the context of the ROC curves in their Figure 1, justified as easing interpretation. However, smoothing is also used in all AUC-ROC values they report. There is no difficulty to be overcome in using non-smoothed plots. Indeed, such plots are standard practice in conflict research and forecasting, both for visualization and when calculating AUCs. We examined all 63 references in B\&S and also 15 articles in two randomly picked issues of JCR for 2020 (numbers 1 and 9), in order to survey the use of ROC curves. Of the 37 articles that used a binary outcome, 19 included either ROC curve figures or a table with AUC-ROC values. We found that 90%--17 of 19--clearly used empirical ROC curves, and 2 (10%) where we could not establish whether empirical or smoothed curves were used.\footnote{One paper reports AUC-ROC values but no figure with ROC curves, and we could not find publicly posted replication code. The other is an ambiguous case where ROC curves are manually constructed in a way that might be considered "smoothing"--and one of the two figures certainly looks smoothed--but it's not a parametric smoothing method like used in B\&S. The online replication archive has more details at https://github.com/andybega/Blair-Sambanis-replication/tree/master/journal-survey. Both of the ambiguous cases include Blair as coauthor.} The survey results are not surprising since the original ideal as well as contemporaneous documentation suggest this is the gold standard.  What is surprising is that anyone would choose the smoothed ROC implementation.

The next three issues we encountered all concern information in B&S Table 2, which is the key table in which B&S report AUC-ROC values to demonstrate the "Escalation" models superiority.   

## Incorrect "Weighted by PITF" Implementation

The "Weighted by PITF" model is described as follows in B&S, page 19:

> The [Weighted by PITF model] uses PITF predicted probabilities to weight the results of the escalation model, ensuring that high-risk and low-risk countries that happen to take similar values on ICEWS-based predictors are nonetheless assigned different predicted probabilities in most months.

We believe that B&S intend that the escalation model's predictions for the test set are weighted by the PITF model predictions for the test set. However, they actually weight the \textbf{test} set predictions using the PITF model predictions for the \textbf{training} set. This appears to be an easily corrected coding error on the part of B&S. 

## Incorrect "PITF Split Population" Implementation

The "PITF Split Population" model also appears to be incorrectly implemented, owing to a coding error. B&S describe it on page 20:

> The final approach is a random forest analog to split-population modeling. We first compute the average PITF predicted probability for each country across all years in our training set. We define those that fall in the bottom quartile as "low risk" and the rest as "high risk." We then run our escalation model on the high-risk and low-risk subsets separately, combining the results into a single random forest [...].

This description suggests that B&S intended to run two separate random forest models, one each on the low- and high-risk training data splits. The replication code does indeed run two separate random forecasts, but they both utilize the *exact same training data*, which consist of the full training data from all other models. In short, rather than using only data for high-risk countries to train their split-sample model for high-risk cases, the data they use captures both high- and low-risk countries, similarly for the training set for their low-risk split. 
The model specifications are also identical; i.e., they use the same *x* variables and the same random forest hyper-parameter settings. The *only* difference in the models as they are implemented in the B&S replication code is due to the non-deterministic nature of the random forest model itself. If we ran both with the same random seed, they would be identical in every respect, producing two forests of identical decision trees and, thus, identical predictions. 

The implementation error aside, this split-population analog model is quite odd and does not replicate the idea behind split-population modeling [@chiba:etal:2015,@beger:etal:2017]. Although the two RFs are trained on separate data (in our updated, fixed replication), the process of combining them actually creates a new, larger RF using both component model's underlying decision trees. Thus, while all RF models throughout (except for one of the robustness checks) are trained with 100,000 decision trees (`ntree`), the new RF model after combination does have 200,000 decision trees. Furthermore, the PITF model predictions do not impact how the combined RF model predicts at all, not even through a binary low-/high-risk split. The split-population PITF RF model is practically speaking simply another escalation model trained with N=200,000 instead of N=100,000 trees and an extra odd randomization step added to the already existing RF randomization facilities (row and column sampling for each decision tree). This does not adequately implement their split-sample modeling strategy.

## Inconsistent test set N for the models in Table 2

Further, the AUC-ROC values reported in the original B&S Table 2 are calculated  on slightly different numbers of underlying test set cases (see Table \ref{tab:table2-N}). ROC calculations for a set of predictions can only be done on the set of cases for which both non-missing predictions and non-missing outcomes are available. Those sets differ across models (columns) for each row in B&S Table 2. 

Thus a difference in AUC-ROC values for two models could be because they were calculated on different sets of underlying cases, not because the models are systemically performing at a different level. In other words, the results for different models in B&S Table 2 are not comparable to one another, and conclusions drawn from such comparison are potentially incorrect. 

## Incorrect scoring of the 2016 forecasts

B&S present a confusion matrix to score their 2016-H1 forecasts in Table 4. Although the forecasts are for the probability of civil war _onset_, in the replication code, they are actually scored using _incidence_ of civil war. By definition the incidence of civil war is much more common than onset, which is only coded for the starting year of a conflict. When forecasting rare events, it is common to have many false positives, i.e. instances where we predict onset but no onsets occur. _Ex ante_, using incidence rather than onset to score onset forecasts should decrease the number of false positives and correspondingly increase the number of true positives (good predictions) and false negatives (missed onsets). 

# Results of the updated analysis

We now turn to an examination that addresses and fixes the five issues discussed above.^[The code for all of our analysis undertaken for this effort may be found at https://github.com/andybega/Blair-Sambanis-replication.] The main results of the original analysis consist of the comparison of the Escalation to other ICEWS models (our Table \ref{tab:table1-nosmooth}, B&S Table 1), and a comparison of the Escalation model to models that add structural variables/information (our Table \ref{tab:table2-fixed}, B&S Table 2). We will review the substantive implications of our updated analysis below, but the bottom line is that these changes turn B&S's conclusions on their heads.

## Smoothed ROC curves and AUC calculations

A reference to smoothing is made in a single sentence in B&S (p. 12):

> Figure 1 displays the corresponding ROC curves, smoothed for ease of interpretation.

This suggest that using smoothed ROC curves will have no substantive impact on the results; however, in this smoothing determines the comparative results.
Their statement also implies that the ROC curves were only smoothed in the referenced Figure 1. However, this is not the case. All AUC-ROC calculations throughout the replication code use an option to smooth the ROC curves *prior* to AUC calculation. ROC curves are constructed from the false positive and true positive rates as one moves through a set of ranked predictions, and as a result they appear step-like. Figure \ref{fig:rocs} shows our replication of both the estimated smoothed ROC curves from the B&S report (left-hand side) and the actual empirical ROC curves (right-hand side). The standard method is to compute the area under the curve (AUC) statistic on the original, empirical ROC curves that are shown on the right. 

\begin{figure}
\caption{Replication of B\&S Figure 1 with both smooth and non-smoothed ROC curves\label{fig:rocs}}
\centering
\includegraphics[width=.95\linewidth]{figures/figure1-replicated.png}
\end{figure}

The specific predictions also include groups of cases with identical predicted probabilities, which accounts for the unusual diagonal lines seen in the panels on the right. In any case, with a sparse outcome like civil war onsets, the true positive rate on the *y*-axis only changes when the prediction for an observed positive case is reached. For these ROC curves, and for that matter in the fundamental train/test split used for 12 of the 18 rows/models in B&S Table 1, there are only 11 civil war onset cases in the test set. Thus, the ROC curves here are very step-like, with only 12 (11 positive cases plus 1 for TPR = 0) distinct *y* coordinates. Notice also that the smoothing averages the left-most almost straight line with the right-most almost straight line in a monotonic way.  Thus, smoothing changes the ordering of alternatives. It is not a cosmetic change as suggested.

## The Effect of Using Smoothed ROC Curves

What impact did the ROC smoothing have overall on the evaluated performance of the Escalation model relative to other ICEWS models (our Table \ref{tab:table1-nosmooth}, B\&S Table 1) and structural extensions (our Table \ref{tab:table2-fixed}, B\&S Table 2)?  Figure \ref{fig:benefit-plot} shows the changes in AUC-ROC values had we used smoothed ROC curves to calculate the AUC-ROC values. Each point corresponds to the change in AUC-ROC values for one of the models in the cells in Tables \ref{tab:table1-nosmooth} and \ref{tab:table2-fixed} (the colors match those in Figure \ref{fig:rocs}). The vertical bar in each plot marks the average effect of smoothing on AUC-ROC. For all alternative models, from Quad to "PITF Only", smoothing sometimes hurts and sometimes benefits, but the overall impact is negligible on average. The Escalation model _always_ benefits from smoothing, with an average improvement on the order of 0.05. This is sufficient to push the Escalation model ahead of the alternative models, and accounts for the incorrect result B&S report, namely that the escalation model is generally superior. 

```{r benefit-plot, fig.height=3.5, fig.width=5.5, out.width = ".8\\linewidth", fig.align = "center", fig.cap = "Gain from using smoothed ROC to calculate AUC, for each model reported in Tables 3 and 4 (B\\&S Tables 1 and 2). The Escalation model is the only model which consistently benefits from using smoothed ROC curves, and this advantage accounts for its apparent superiority to the other models."}
table1_benefit <- read_csv("data/table1-smooth-benefit.csv") 
table2_benefit <- read_csv("data/table2-smooth-benefit.csv") 

cols <- c(
  Escalation = "#e41a1c",
  Goldstein = "#ff7f00",
  Quad = "#984ea3",
  CAMEO = "#377eb8",
  Average = "#4daf4a",
  `With PITF Predictors` = "gray50", 
  `Weighted by PITF` = "gray50", 
  `PITF Split Population` = "gray50",
  `PITF Only` = "gray50"
)

benefit <- bind_rows(
  table1_benefit %>%
    rename(setting = Model) %>%
    pivot_longer(-c(horizon, setting), names_to = "model", values_to = "diff"),
  table2_benefit %>%
    rename(setting = Model, Escalation = `Escalation Only`) %>%
    pivot_longer(-c(horizon, setting), names_to = "model", values_to = "diff")
)

# mean benefit by model
means <- benefit %>%
  group_by(model) %>%
  summarize(avg_benefit = mean(diff))

ann_text <- tibble(horizon = "1 month", diff = 0.07, model = 8.6, label = "Mean")

model_levels <- c("Escalation", "Quad", "Goldstein", "CAMEO", "Average", 
                  "With PITF Predictors", "Weighted by PITF", "PITF Split Population",
                  "PITF Only")
benefit %>%
  mutate(model = factor(model, levels = rev(model_levels))) %>%
  ggplot(aes(y = model)) + 
  #facet_wrap(~ horizon, scales = "free_x") +
  geom_point(aes(x = diff, color = model), alpha = 0.5, size = 2) +
  scale_color_manual(guide = FALSE, values = cols) +
  geom_point(data = means, aes(x = avg_benefit), shape = "|", color = "black", size = 6) +
  labs(x = "AUC(smoothed ROC) - AUC(empirical ROC)",
       y = "") +
  theme_bw() +
  geom_text(data = ann_text, aes(x = diff, label = label), col = "black") +
  geom_vline(xintercept = 0, linetype = 3) +
  theme(axis.text.y = element_text(size = 12))
```

In sum, it is not only the case that using smoothed ROC curves alters the results, but also that the use of smoothed ROC curves to calculate AUC-ROC values benefits \textbf{only} the escalation model. It does so consistently and by a considerable margin. All eight alternative models reported in B&S Tables 1 and 2, on average, do not gain when using smoothed ROC curves to calculate AUC. Four of the eight show a small positive bias from smoothing, while the other four display a small negative bias. The magnitudes of these are not close to the level of positive bias found in the escalation model.

## Is the Escalation model superior to the alternative ICEWS models?

Table \ref{tab:table1-nosmooth} (B&S Table 1) shows the comparison of the Escalation model to other alternative models based on ICEWS event data indicators. The AUC-ROC values are based on original, non-smoothed ROC curves. 
Both the Average and CAMEO models outperform the Escalation model in almost all instances (see also Figure \ref{fig:table1-plot}). The Goldstein model generally outperforms the Escalation model in the 6-month version. The Quad model appears to be roughly on par with the Escalation model. Thus, the original B&S conclusion that the Escalation model is superior to the alternative models is entirely conditional on the non-standard use of smoothed ROC curves.

```{r table1-nosmooth}
table1_nosmooth <- read_csv("data/table1-nosmooth.csv")

table1_nosmooth %>%
  select(-horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Comparison of the escalation model to alternative ICEWS model using test set AUC-ROC, \\textit{without} smoothed ROC curves (Replication of B\\&S Table 1)",
    label = "table1-nosmooth") %>% 
  pack_rows(index = c("One-month forecasts" = 9, "Six-month forecasts" = 9))
```

The key difference between Table \ref{tab:table1-nosmooth} and B&S Table 1 is whether the underlying ROC curves were original or smoothed; the test set N and coding issues did not affect this set of results.^[We should note that in our replication we consciously decided to not set RNG seeds, even though the random forest models are non-deterministic and vary slightly from run to run. As we changed the replication code to allow running in parallel, we cannot exactly reproduce the B&S results even with the same RNG seed. More importantly, the substantive interpretation of results should not depend on a specific RNG seed. We found that the escalation model's AUC-ROC values generally fluctuate no more than 0.01 (see https://github.com/andybega/Blair-Sambanis-replication/blob/master/rep_nosmooth/variance.md), and thus are confident that the patterns we see are robust to the initial RNG state.] B&S's original results and interpretation regarding the superiority of the B&S model over alternative ICEWS models, including the 1,000+ covariate CAMEO model, are thus entirely conditional on the use of smoothed AUC-ROC. Even the Quad model is generally as good as the escalation model in all implementations and is frequently quite a bit better.

Aside from the visual impact of ROC smoothing, the underlying motivation and methods involved in smoothing are broader and involve parametric estimation of population ROC curves [@henley:1988; @henley:2014]. The main application is in the evaluation of medical tests (e.g. diagnostic imaging) with early methods dating back several decades and ongoing development of both parametric and non-parametric smoothing methods [e.g. @zou:etal:1997; @pulit:2016].  It is not established which methods are preferable in a given application [@henley:2014]. Notably, the context in which ROC smoothing is used differs in important aspects from typical conflict research applications. Conflict data are closer to a census of the population and with well-known dependencies like spatial and temporal correlation, rather than a random sample that is approximately independently and identically distributed. It is thus neither clear that smoothing is justified nor valid. We also note that smoothed ROC plots are not widely used outside of the narrow context mentioned above.

## Using Inconsistent Ns

The top portion of Table \ref{tab:table2-N} shows the number of valid test set predictions that can be scored for each model in the original B&S Table 2. For the 1-month data version, the number of predictions differs by up to 500 cases, and in the 6-month data version by around 100 cases. These numbers appear small enough to be not important. However, the already small number of positive cases also is affected substantially:  "With PITF Predictors" and "PITF Only" models lose two or one (respectively) of 11 positive cases in the 1- and 6-month data versions. 

```{r}
table2_full <- read_csv("data/table2-for-appendix.csv")

table2_full %>%
  select(horizon, column, N_test, cases) %>%
  pivot_wider(names_from = column, values_from = N_test) %>%
  arrange(horizon, desc(cases)) %>%
  rename(Cases = cases, Horizon = horizon) %>%
  select(Cases, Horizon, everything()) %>%
  arrange(desc(Cases), Horizon) %>%
  knitr::kable(
    "latex", booktabs = TRUE, digits = 0,
    caption = "Number of valid test predictions for the escalation and structural alterantives models (B\\&S Table 2)",
    label = "table2-N") %>%
  column_spec(3:7, width = "2cm") %>%
  collapse_rows(1:2, row_group_label_position = "stack") 
```

Generally, these comparisons require the same cases.  Therefore, we use predictions for a common joint subset of non-missing predictions. The resulting numbers of cases for each model are shown in the bottom portion of the table. Since the sets of incomplete cases in the original version above do not entirely overlap themselves, the common subset for all models is slightly smaller than the smallest N in the top portion of the table.

## Do Structural Variables Add to the Escalation Model's Predictive Power?

Table \ref{tab:table2-fixed} shows our replication of B&S Table 2 with (1) regular, not smoothed, AUC-ROC, (2) fixed "Weighted by PITF" and "PITF Split-Population" models, and (3) AUC-ROC values computed on the common, joint subset of tests cases for which all models have non-missing predictions. Table \ref{tab:table2-full} in the appendix shows AUC-ROC values for both smoothed and non-smoothed versions, and both the original, model-varying test cases sets and our common joint subset. 

```{r table2-nosmooth}
table2_nosmooth <- read_csv("data/table2-nosmooth.csv")

table2_nosmooth %>%
  dplyr::select(-Model, -horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Comparison of escalation model to structural extensions, using test set AUC-ROC (Replication of B\\&S Table 2)",
    label = "table2-fixed") %>%
  column_spec(2:6, width = "2cm") %>%
  pack_rows(index = c("One-month forecasts" = 1, "Six-month forecasts" = 1)) %>%
  footnote("Differences from the original B&S Table 2: (1) AUC-ROC values are computed on the common subset of cases, meaning that N is equal in each row; (2) AUC-ROC values are computed using original, non-smoothed ROC curves.", footnote_as_chunk = TRUE, threeparttable = TRUE)
```

B&S interpret their results as follows (page 20) and we comment on the conclusions seratim:^[We list the "Overall, ..." interpretation out-of-order for clarity.]

1. B&S: "Of the approaches we test, the split-population analog is most promising ..."

The PITF split-population analog still performs well, but the simple Escalation + PITF predictors model arguably performs better still. 

2. B&S: "Adding PITF predictors improves the performance of the escalation model over six-month windows but diminishes it over one-month windows \ldots "

Adding PITF predictors actually improves performance in both cases; the "With PITF Predictions" model strictly dominates the "Escalation Only" model by quite a margin in the 6-month data version.  

3. B&S: "The weighted model performs very poorly regardless \ldots "

The weighted model performs roughly on par with the Escalation Only model. One finding that remains is that the "PITF Only" model is outperformed by the "Escalation Only" model. As the former only uses annual inputs, but the data at hand are the 1-month or 6-months level, this is neither surprising, nor especially noteworthy.

4. B&S: "Overall, our results suggest that while measures of structural risk may improve predictive performance, the value they add is marginal and inconsistent. [...] Incorporating PITF thus significantly reduces or only slightly improves the performance of the escalation model, regardless of the approach we take. [...] "

The most straightforward method of incorporating the annual structural PITF variables---adding them to the predictors of the Escalation RF model---strictly outperforms the Escalation Only model. Note that the two other combination models considered are both non-standard and that the "PITF Split Population" model does not incorporate structural information at all, yet they also both do well. We thus conclude that adding structural variables improves predictive performance.

## How Accurate Were the 2016 Forecasts?

B&S made forecasts for the first half of 2016 (2016-H1), and assess the accuracy of these forecasts. The forecasts themselves are probabilistic, i.e. range from 0 to 1, but for the purpose of scoring the forecasts they consider the 30 highest predictions to have been positive forecasts ("1") and the remaining forecasts to have been for no onset ("0"). They then construct confusion matrices for the forecasts, in their Table 4 and which we replicate here in the top half of our Table \ref{tab:table4}. They use two alternative codings of the outcomes for 2016-H1, one of which they call "Assuming Persistence" (this is what is in their replication data), and one of which they call "Assuming Change" (hand-coded in one of their replication scripts). 

With the "Assuming Persistence" version of the outcomes, shown in the first confusion matrix there are 15 civil war onsets, of which the forecasts capture 13 (true positives) and miss 2 (false negatives). With the "Assuming Persistence" coding of outcomes, shown in the second confusion matrix, there are 16 onsets and the forecasts capture 14 of those with, still, 2 misses. Fifteen to 16 civil war onsets in the first half of 2016 is incredible, especially considering that the entire historical data from 2001 to 2015 only has 20 recorded civil war onsets. Indeed, it turns out that B&S scored their forecasts for civil war \textit{onset} using civil war \textit{incidence}, i.e. counting ongoing civil wars as if they were continuing onsets. 




```{r, include=FALSE}
# AB 2020-09-22: I redid the confusion matrices in latex so that the resulting table is clearer than what I could easily do with kable() programatically here in this chunk. 
tab4o <- read_csv("data/table4-original.csv")
tab4f <- read_csv("data/table4-fixed.csv")

tab4o <- tab4o %>%
  rename(header = type) %>%
  mutate(source = "Original, scored with civil war incidence")
tab4f <- tab4f %>%
  rename(Predicted0 = `0`, Predicted1 = `1`) %>%
  mutate(source = "Fixed, scored with civil war onset")

tab4 <- bind_rows(tab4o, tab4f) %>%
  select(source, header, everything())

tab4 %>%
  knitr::kable(
    caption = "Replication of B\\&S Table 4: 2016 Confusion Matrices for Six-month Escalation Model.",
    label = "table4",
    booktabs = TRUE
  ) %>%
  kableExtra::collapse_rows(1:2, row_group_label_position = "stack")
```


\begin{table}
\caption{Replication of B\&S Table 4: Confusion Matrices for Six-month Escalation Model forecasts for the first half of 2016.}\label{tab:table4}
\centering
\begin{tabular}{ll|c|c|l}
\multicolumn{5}{c}{\textbf{Original confusion matrices with civil war \textit{incidence}}} \\
% BS, assuming persistence
\multicolumn{1}{l}{\multirow{4}{*}{Assuming Persistence}} & \multicolumn{3}{c}{} \\
\multicolumn{2}{c}{}& \multicolumn{1}{c}{$P=0$} & \multicolumn{1}{c}{$P=1$} \\
\cline{3-4}
& $Y=0$ & 132 & 17 \\
\cline{3-4}
& $Y=1$ & 2 & 13 \\
\cline{3-4}
% BS, assuming change
\multicolumn{1}{l}{\multirow{4}{*}{Assuming Change}} & \multicolumn{3}{c}{} \\
\multicolumn{2}{c}{}& \multicolumn{1}{c}{$P=0$} & \multicolumn{1}{c}{$P=1$} \\
\cline{3-4}
& $Y=0$ & 132 & 16 \\
\cline{3-4}
& $Y=1$ & 2 & 14 \\
\cline{3-4}
\\ \midrule
\multicolumn{5}{c}{\textbf{Fixed confusion matrices with civil war \textit{onset}}} \\
% fixed, assuming persistence
\multicolumn{1}{l}{\multirow{4}{*}{Assuming Persistence}} & \multicolumn{3}{c}{} \\
\multicolumn{2}{c}{}& \multicolumn{1}{c}{$P=0$} & \multicolumn{1}{c}{$P=1$} \\
\cline{3-4}
& $Y=0$ & 134 & 30 \\
\cline{3-4}
& $Y=1$ & 0 & 0 \\
\cline{3-4}
% fixed, assuming change
\multicolumn{1}{l}{\multirow{4}{*}{Assuming Change}} & \multicolumn{3}{c}{} \\
\multicolumn{2}{c}{}& \multicolumn{1}{c}{$P=0$} & \multicolumn{1}{c}{$P=1$} \\
\cline{3-4}
& $Y=0$ & 134 & 28 \\
\cline{3-4}
& $Y=1$ & 0 & 2 \\
\cline{3-4}
\end{tabular}
\end{table}


The correct confusion matrices that use civil war onsets to score the forecasts are shown in the second part of Table \ref{tab:table4}. In the "Assuming Persistence" coding, there are no civil war onsets in the data for 2016-H1. The alternative "Assuming Persistence" coding records 2 civil war onsets in 2016-H1. As a result, if we compare the precision and recall of the forecasts with the correct scoring to the incorrect scoring in B&S, almost all of the forecasts are false positives. This is the norm when predicting rare events, and thus not unusual at all. The forecasts do correctly capture the 2 civil war onsets in the "Assuming Change" version of the outcome coding. This is, at face value, a credit to the model's accuracy. 

# Discussion

B\&S advocate the use of theory to guide prediction. But "theory" is not defined by B\&S and may be an ambiguous and undefined concept. "Theory" has many interpretations in social science.  For some---the EITM movement, for example [@aldrich:etal:2008]---theory is a system of internally consistent, logically connected propositions from which additional statements may be deduced.  The additional statements can be subjected to empirical scrutiny.  An example might be William Riker's theory of coalitions [@riker:1962] which posits propositions in game theoretic form from which the size principal of minimal winning coalitions may be deduced and further examined empirically. A more recent example of strong theory is found in *The Political Logic of Survival* [@bdm:etal:2005]. For the purposes of discussion we call this strong theory.

However, for many, a theory is a falsifiable story or narrative that makes causal claims about the relationships among variables. Typically it is fused together from a meta-analysis of prior work on the topic under consideration. We call these types of theory weak theory.
They are weak because they do not include two characteristics of strong theory, though they may include one:

1. A theory is separate from the case or cases being examined. It is intended to be general.
2. A theory generates new statements that in principle can be empirically falsified, not proven.

Most social science research uses a weak form of theory.  In the case of B\&S they base their theoretical thinking on research that argues that conflict is not the product of structural factors, but rather comes from an interaction of state repression and the changes in costs and benefits to citizens for engaging in anti-government conflict. Following a wide swath of scholars, they argue that the escalation and de-escalations of interactions between citizens and government are what typically lead up to the onset of civil wars.  Operationally, this boils down to four types of concepts: demands, accommodations,
nonviolent repression, and low-level violence.

\begin{itemize}
\item Demands include events in which either opposition or rebels is the source and
government is the target and typically involve appeals for political or institutional
reform, policy change, or extension of rights.
\item Nonviolent repression and accommodation include events in which government
is the source and either opposition or rebels is the target. Nonviolent
repression includes threats, sanctions, restrictions on freedom of movement, or
rejections of group demands.
\item Low-level violence includes all events in which
(1) government is the source and opposition is the target, (2) opposition is the
source and government is the target, (3) government is the source and rebels are
the target, or (4) rebels are the source and government is the target.
\end{itemize}

These variables, taken from ICEWS, are the main components of the escalation model in the B\&S article. The escalation theory is that the forces behind these variables interact such that as each of them increases, they independently increase the probability of civil conflict.  The actual mechanisms of the dynamism among these variables are not specified.

Indeed most empirical social science research follows this approach: 1) delineation of an argument, 2) selection of variables that represent a plausible or compelling representation of the concepts, followed by 3) utilization of these variables in a statistical model with a dependent variable that measures the main phenomena of investigation. It is worth pointing out that this is simply one set of variables that are selected to represent the concepts in the theoretical story. A strong theory gives more guidance on compelling choices than does a weak theory.  Accordingly, while the choice is plausible is not necessarily compelling.  Another choice might be equally plausible.  In the ICEWS example, we might include variables on intragovernmental demands (from the military toward the central government), or further look a the dynamics among opposition groups.    In practical terms there is a wide array of choices that are available to instantiate any particular theory.  A second point is that putting them in a linear regression may be a particularly crude instantiation of complicated dynamics.  One general principal that is on point in the B\&S article and is more widely relevant is that dynamical phenomenon like civil wars need to have explanatory models that change rapidly over time---and by implication that are represented by data that change fairly quickly.

At this level of detail the theoretical model is more about choosing the correct variables and this is the contrast between theoretical prediction models and atheoretical machine learning that B&S focus on. But this dichotomy is misguided. The data that are used in a predictive model have to come from somewhere. Even if the inclusion of some factors is not explicitly justified, it is hardly the case that one assembles data without the expectation that it will be useful. It is not a coincidence that nobody uses 1,180 variable strawman models whose variable vectors predominantly consist of 0's, even if such a model coincidentally ends up performing well. Rather, we posit that most of the cases B&S might characterize as atheoretical enter the "theoretical", with some awareness of existing arguments and work that is used to select data sources for inclusion in a dataset.\footnote{While we do not undertake this, there are a variety of procedures that one might use for variable choices, ranging from staring at lists of variables to machine learning approaches.}

Furthermore, if what we mean with "theory" is such that it suggests a large range of variables that could be related to an outcome, what do we really gain from having this theory in our predictive model? We can construct a model designed to predict well while avoiding overfitting, and then go back and try to couch it in theoretical language---which, given the malleability of extant theory, we probably could in many cases---but are we then better off? Certainly that's not what B&S are trying to suggest since this would be a problematic way to build a theory. But when theory is extensive or malleable so that it suggests a broad range of variables and specifications, and thus a broad range of specifications could post fact be tied to a theory, it is hard to demonstrate that this theory is in fact not the result of predictive modeling's equivalent to p-hacking, namely developing a model specification and any associated "theory" with any eye towards out-of-sample predictive accuracy [cf. mahmood colarsei paper on how to do this properly though]. Incidentally, this is also a recipe for creating overfit models whose accuracy does not generalize, and whose underlying theories are by extension not externally valid [zones farris paper].

Conversely, not all factors that are "theoretically" important or suggested by the literature are also important for prediction, and some factors that turn out to be good predictors do not originate from prior theory. For example, both @wgb:2007 and @hill:jones:2014 examine a variety of variables that are considered "theoretically" important for explaining civil war onset and state repression, respectively, and find that there is substantial variation in how useful they are for for actually predicting those outcomes. And it is entirely possible that a focus on building accurate predictive models could uncover factors overlooked in the literature, e.g. the strong association between infant mortality and a broad variety of conflict outcomes like political instability [@goldstone:etal:2010], irregular leadership changes [@beger:etal:2017], and coups.^[Andreas Beger and Michael D. Ward, 2020, “Coup forecasts for 2020”, https://www.predictiveheuristics.com/forecasts]

% MIKE: is it actually true that infant mortality has mostly not been studied in the context of explaining conflict risk? Goldstone in their article claim that it has, but for me that's news. 

Aside from selection of the variables that go into a model, another way in which we could contrast theoretical from atheoretical prediction models is in terms of their flexibility in regard to functional form. Theory should not only tell us what factors matter, but also how they relate to each other. We think that few would be strongly committed to the idea that the linear additive equations that dominate published regression analyses, maybe with the occasional polynomial or multiplicative interaction terms, are *the* correct functional specification. Is the danger posed by low-level violence the same when tensions are high, with high demands and high non-violent repression, as when tensions are low? Are those relationships linear in the log-odds? Some machine learning models, random forests included, are flexible in this regard and to some extent capable of learning non-linear and interactive relationships. B&S implicitly concede on this point since they use random forests for both their Escalation model and the atheoretical alternatives. But in the absence of strong a priori theoretical expectations about functional form, why not use machine learning tools to try to uncover any such non-linear relationships in need of explanation, and use this to improve our theory?

Rather than trying to frame theoretical modeling and machine learning as competing endeavors, a more useful perspective is to consider the ways in which they can mutually reinforce each other. Even if the analysis in B&S fails to support the point, we concede that predicting on the basis of a strong theoretical model is preferable to inductive prediction. Beyond accurate predictions, such models would allow us to make causal statements that are harder to make on the basis ad hoc models. But this is hard to do in practice. The problem is that many extant theoretical models are in fact so malleable to interpretation that they essentially are ad hoc. They are useful for identifying variables that could be useful for prediction, and this is in fact already taking place.

Most importantly, a single study is unlikely to be convincing that theory-based models are better (or worse) than ad hoc models based mainly in machine learning.  Even if B\&S were correct that in their study the theory based escalation model performed better, it would simply be a case study of a single case where it was preferable. It establishes no general tendency, let alone law-like evidence for general theory. Nor, against it.

# Conclusion

We encountered several issues in the code underlying the B&S analysis. The problems we encountered are not subjective modeling choices.  They bear no implications on the question of whether theory based model are preferable.  However,  when we fix these issues and perform an updated analysis, the B&S conclusions are all essentially overturned. In other words, B&S findings are based on a faulty analysis, invalid, and should be disregarded as evidence in favor of theory-based models. 

In contradistinction to the conclusions offered by B&S, we find that when correctly specified and implemented:

- The theory-driven escalation model is outperformed both by the low-effort 1,160 predictor all-CAMEO model and the ensemble average model.
- The Average ensemble model and the CAMEO models outperform the escalation model in all instances.
- Adding structural variables substantially improves the escalation model's performance.

More importantly, trying to contrast machine learning and theory may be misguided. Even if a "theory" section is missing, predictive models are informed by previous work and arguments. If we had strong general theories of something like civil war onset, we could replace more inductive, ad hoc, methods with cleary specified models. But we do not. What we have instead are weak theories that could be interpreted to suggest any range of equations for predicting an outcome. Instead of trying to find an instantiation of such a "theoretical" model that can outperform a more ad hoc alternative, we could use the flexibility of machine learning methods and insights that predictive modeling brings to improve our weak theories. 

Blair & Sambanis have focused attention on comparing forecasting models and forecasts in the civil war domain.  As social science becomes more adept at predictive analysis, this will doubtless be of increasing importance.  However, these comparisons must be made carefully to ensure those correct inferences are drawn.  We continue to think that weak theory is overrated [@ward:2016], and that machine learning and big data will allow us to learn new things. It will be interesting to see how the evidence for these claims is adjudicated with additional usage and careful evaluation.  Social science has leaned on theory for (at least) seventy years, but it might be time to find useful ways to incorporate some of the recently developed methods in the arena of machine learning and modern statistics as well.

\newpage

# References

<div id="refs"></div>

\newpage

# (APPENDIX) Appendix {-}

# Appendix

\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}

\renewcommand{\thetable}{A\arabic{table}} 
\setcounter{table}{0}

## Code references for coding errors

### Section 3.2: Incorrect “Weighted by PITF” Implementation

We believe the intention with the "Weighted by PITF" model is to combine the PITF test predictions with the Escalation test predictions, i.e. weight the latter with the former. Due to an error, the PITF _training_ set predictions are used, not the _test_ set predictions. 

See `1mo_run_escalation_weighted_PITF.R` line 4, where the PITF predictions are taken from the training data set (`train$pred_prob_plus1`). The next line is a hack extending the shorter `weight` vector with missing values to avoid a R warning when it is multiplied with the longer vector of escalation model test set predictions. Similarly in the 6-month version of this file.

Additional details in issue #3 in the GitHub repo. 

### Section 3.3: Incorrect “PITF Split Population” Implementation

The intention is to train seperate random forests on data split into high and low-risk groups based on PITF predictions, and then combine them back into one random forest. Actually both random forests are trained on identical data, essentially production a random forest that just has N_trees \* 2. 

Disentangling this coding error is not straightforward as it occurs over several R scripts and requires (or at least is easier to verify by) running partway through the actual replication until the objects holding the training data for the models are instantiated and can be examined. Details are documented in issue #5 in the GitHub repo.

### Section 3.5: Incorrect scoring of the 2016 forecasts

The relevant variables in the data are "incidence_civil_ns" and "incidence_civil_ns_plus1", which appears to be a 1-period lead (ie., $t_+1$) version of the dependent variable that is used in the actual prediction models. The incidence dependent variable  contains both 0/1 and missing values. By examining the pattern of missing values, it seems clear that this was originally an incidence variable indicating whether a country was at civil war in a given year or not, and which was converted to an onset version so that onsets retain the value of 1 but continuing civil war years are coded as missing. This reflects common practice. 

However, by examining the code used to generated Table 4, we were able to confirm that the onset forecasts are assessed using incidence, not onset. In the file `6mo_make_confusion_matrix.do` on line 52, missing values in "incidence_civil_ns" are recoded to 1, thus reverting the onset coding of this variable to incidence. 

## Additional replication tables

Table \ref{tab:table1-smooth} is our replication of B&S Table 1 with smoothed AUC-ROC. The results differ slightly from the original B&S Table 1, typically by no more than 0.01, due to the non-deterministic nature of the RF models. It is the case that B&S set the RNG seed in their replication code, which should theoretically allow exact reproduction, but (1) there was a change in more recent versions of R that affected the RNG seeding process, and (2) we refactored the replication script to allow one to run the models in parallel. In any case, the interpretation of results should not be sensitive to random variation, i.e. it should not depend on using a specific RNG seed. On the basis of these results, B&S conclude that the escalation model is generally superior to the alternatives, and we can replicate that interpretation when using smoothed ROC curves. 

```{r table1-smooth}
table1_smooth <- read_csv("data/table1-smooth.csv")

table1_smooth %>%
  select(-horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Replication of B\\&S Table 1 with smoothed ROC curves; test set AUC-ROC for various models",
    label = "table1-smooth") %>% 
  pack_rows(index = c("One-month forecasts" = 9, "Six-month forecasts" = 9))
```

```{r}
table2_full <- read_csv("data/table2-for-appendix.csv")
  
table2_full %>%
  pivot_longer(auc_roc:auc_roc_smoothed, names_to = "Smoothed ROC",
               values_to = "auc_roc") %>%
  mutate(`Smoothed ROC` = case_when(
    `Smoothed ROC`=="auc_roc" ~ "No",
    TRUE ~ "Yes")
  ) %>%
  select(horizon, column, cases, `Smoothed ROC`, auc_roc) %>%
  pivot_wider(names_from = column, values_from = auc_roc) %>%
  rename(Cases = cases, Horizon = horizon) %>%
  select(Cases, Horizon, `Smoothed ROC`, everything()) %>%
  arrange(desc(Cases), Horizon, desc(`Smoothed ROC`)) %>%
  knitr::kable(
    "latex", booktabs = TRUE, digits = 2,
               caption = "Replication of B\\&S Table 2 with smoothed/original ROC and with original varying N cases or adjusting for common case set with constant N",
    label = "table2-full") %>%
  column_spec(3, width = "1.5cm") %>%
  column_spec(4:8, width = "2cm") %>%
  collapse_rows(1:3, row_group_label_position = "stack")

```

```{r smooth-benefit-tables}
table1_benefit <- read_csv("data/table1-smooth-benefit.csv") 
table1_benefit %>%
  select(-horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Smoothing advantage for B\\&S Table 1: the gain in AUC-ROC when calculated using smoothed ROC curves",
    label = "table1-benefit") %>% 
  pack_rows(index = c("One-month forecasts" = 9, "Six-month forecasts" = 9))

table2_benefit <- read_csv("data/table2-smooth-benefit.csv") 
table2_benefit %>%
  select(-horizon) %>%
  knitr::kable(
    digits = 2, booktabs = TRUE, 
    caption = "Smoothing advantage for B\\&S Table 2: the gain in AUC-ROC when calculated using smoothed ROC curves",
    label = "table2-benefit") %>% 
  column_spec(2:6, width = "2cm") %>%
  pack_rows(index = c("One-month forecasts" = 1, "Six-month forecasts" = 1))
```

## Model to model comparison plots

Figures \ref{fig:table1-plot} and \ref{fig:table2-plot} replicate the information in Tables \ref{tab:table1-nosmooth} and \ref{tab:table2-fixed} in a way makes the comparison of the escalation model to the alternative models easier. Each facet shows the escalation AUC-ROC for all model settings (the rows in Table \ref{fig:table1-plot} and base specification for all models in Table \ref{tab:table2-fixed}) on the left, and the AUC-ROC for an alternative model on the right, with a connecting line. If the lines slope up to the right, the alternative model is better. 

```{r table1-plot, fig.height=5, fig.width=8, out.width = ".9\\linewidth", fig.align = "center", fig.cap = "Escalation to alternative comparisons for the ICEWS models (B\\&S Table 1)"}
table1_nosmooth <- read_csv("data/table1-nosmooth.csv")

icews_lvls <- c("Escalation", "Quad", "Goldstein", "CAMEO", "Average")

df <- table1_nosmooth %>%
  rename(Spec = Model) %>%
  pivot_longer(Escalation:Average, names_to = "Model", values_to = "auc_roc") %>%
  mutate(Model = factor(Model, levels = icews_lvls)) %>%
  nest(data = c(Spec, auc_roc))

pairs <- full_join(
  df %>% filter(Model!="Escalation"),
  df %>% filter(Model=="Escalation") %>% rename(data2 = data) %>% select(horizon, data2),
  by = c("horizon")
) %>%
  pivot_longer(data:data2, names_to = "xname", values_to = "data") %>%
  mutate(xname = ifelse(xname=="data", as.character(Model), "Escalation"),
         xname = factor(xname, levels = icews_lvls))

pairs %>%
  unnest(data) %>%
  ggplot(aes(x = xname, y = auc_roc)) +
  facet_grid(horizon ~ Model, scales = "free_x") +
  geom_point() +
  geom_line(aes(group = Spec)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank()) +
  labs(x = "", y = "AUC-ROC (empirical)")
```

```{r table2-plot, fig.height=5, fig.width=8, out.width = ".9\\linewidth", fig.align = "center", fig.cap = "Escalation to alternative comparisons for the structural models (B\\&S Table 2)"}
table2_nosmooth <- read_csv("data/table2-nosmooth.csv")

model_lvls <- colnames(table2_nosmooth)[-c(1:2)]

df <- table2_nosmooth %>%
  rename(Spec = Model) %>%
  pivot_longer(`Escalation Only`:`PITF Only`, names_to = "Model", values_to = "auc_roc") %>%
  mutate(Model = factor(Model, levels = model_lvls)) %>%
  nest(data = c(Spec, auc_roc))

pairs <- full_join(
  df %>% filter(Model!="Escalation Only"),
  df %>% filter(Model=="Escalation Only") %>% rename(data2 = data) %>% select(horizon, data2),
  by = c("horizon")
) %>%
  pivot_longer(data:data2, names_to = "xname", values_to = "data") %>%
  mutate(xname = ifelse(xname=="data", as.character(Model), "Escalation Only"),
         xname = factor(xname, levels = model_lvls))

pairs %>%
  unnest(data) %>%
  ggplot(aes(x = xname, y = auc_roc)) +
  facet_grid(horizon ~ Model, scales = "free_x") +
  geom_point() +
  geom_line(aes(group = Spec)) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(x = "", y = "AUC-ROC (empirical)")
```


## Random forest hyper-parameter selection

What initially sparked our interest in the paper was the unusual choice of hyperparameter settings for the random forest models estimated. Table \ref{tab:hp} shows the default values used by the implementation of random forest that B&S use (from the **randomForest** R package), in contrast to the basic settings used by B&S for most the models reported in the paper. 

As the outcome is a binary indicator of civil war onset, one would typically use a classification random forest that predicts 0 or 1 labels directly. The implementation of random forests that B&S use ([@liaw:wiener:2002]) is based on the original @breiman:2001 implementation and calculates predictive probabilities by averaging over the "0" or "1" votes from all constituent decision trees. The conventional wisdom regarding the number of trees in a random forest is that it needs to be large enough to stabilize performance, but without any additional gain or harm in accuracy beyond a certain number. From the other default settings, which are generally not uninformed choices, one can see that the basic logic is to grow a forest with a relatively small number of trees, but where each tree is fairly extensive, and operates on a large bootstrapped sample of the original training data. These are of course only heuristics and it is usual to attempt to find better hyper-parameter methods through some form of tuning. 

\begin{table}
\caption{\label{tab:hp} Random forest (\texttt{randomForest()}) default versus B\&S hyperparameters}
\begin{tabular}{l>{\raggedright\arraybackslash}p{2in}ll}
\toprule
Hyperparameter & Default heuristic & Default values (Escalation) & B\&S value \\
\midrule
type & & classification & regression \\
ntree & & 500 & 100,000 or 1e6 \\
mtry & \texttt{floor(sqrt(ncol(x)))} & 3 & 3 \\
replace & & true & false \\
sampsize & \texttt{nrow(x)} if replace, else \texttt{ceiling(.632*nrow(x))} & 11,869 & 100 or
500 \\
nodesize & 1 for classification & 1 & 1 \\
maxnodes & & null & 5 or 10 \\
\bottomrule
\end{tabular}
\end{table}

B&S in contrast fit very large forests with 100,000 trees in the basic model form, but where each tree only operates on a very small sub-sample (N=100 or 500), drawn without replacement, of the available training data. This approach only works due to the choice to use regression, not classification, trees. Trying to use classification trees with the other parameter settings does not work at all because it is almost guaranteed that a sample of 100 from the ~11,000 training data rows with 9 positive cases will only include 0 (negative) outcomes in the sample. As it is, using regression with a 0 or 1 outcome produces warnings when estimating the models: 

```
Warning message:
In randomForest.default(y = as.integer(train_df$incidence_civil_ns_plus1 ==  :
  The response has five or fewer unique values.  Are you sure you want to do regression?
```

As it turns out, using regression random forests for this kind of binary classification problem in order to obtain probability estimates matches the probability random forest approach suggested and positively evaluated in @malley:etal:2012, and which is used in another prominent R implementation of random forests.^[The **ranger** package.] It is not clear whether this is intentional, as the Malley paper is not cited in B&S. 

In any case, B&S's random forest approach appears to work really well. We tried to construct classification random forests tuned via cross-validation on the training data set partition, i.e. without touching the test data, but were unable to develop models that consistently match the B&S random forest method in both cross-validated out-of-sample training predictions and test set predictions. 

Given that they are relatively unorthodox, yet appear to work very well, we wonder how the hyper-parameter values were determined. Two specific concerns are that this was not done with an eye towards test set accuracy, which would invalidate the independence of the out-of-sample test set, and whether the specific hyper-parameter values are optimized for only one model, or were optimized and found to work well for all models. There is no discussion of the random forest tuning strategy or how the specific hyper-parameter methods were determined in the paper. 

Given the dramatic changes in results as a result of the preceding issues, we did not further investigate these potential concerns.

## Different smoothing methods

The **pROC** package `smooth()` function includes several different smoothing methods. The default, which BS use in their code, is binormal smoothing. Here is a replication of the smooth benefits shown in Figure \ref{fig:benefit-plot} with two other smoothing methods. (We could not get "logcondens" and "logcondens.smooth" to work, and they are based on another external package that has not been updated since 2016.) 

Only the "binormal" smoothing method produces a pattern of AUC-ROCs that clearly elevates the Escalation model above the alternatives. The "density" method does not on average change the AUC-ROC values very much from their empirical ROC curve equivalents. The "fitdistr" method tends to increase all AUC-ROC values, although maybe slightly more so for the Escalation model than the main alternatives. 

```{r smooth-benefit-plot-extended, fig.height=3.5, fig.width=5.5, out.width = ".8\\linewidth", fig.align = "center", fig.cap = "Gain from using smoothed ROC to calculate AUC, for each model reported in Tables 3 and 4 (B\\&S Tables 1 and 2), with 3 different smoothing methods (binormal is the default method and used in B\\&S)"}

suppressPackageStartupMessages({
  library(tidyverse)
  library(yardstick)
  library(pROC)
  library(forcats)
})

preds <- read_rds("rep_nosmooth/output/all-predictions.rds")

auc_roc_vec <- function(pred, truth, smooth, sm = "binormal") {
  roc_obj <- pROC::roc(truth, pred, auc = TRUE, quiet = TRUE, smooth = smooth, smooth.method = sm)
  as.numeric(roc_obj$auc)
}

rocs <- preds %>% 
  filter(!is.na(value)) %>%
  group_by(cell_id) %>% 
  summarize(
    no = auc_roc_vec(pred = pred, truth = value, smooth = FALSE),
    binormal = auc_roc_vec(pred = pred, truth = value, smooth = TRUE, sm = "binormal"),
    density = auc_roc_vec(pred = pred, truth = value, smooth = TRUE, sm = "density"),
    fitdistr = auc_roc_vec(pred = pred, truth = value, smooth = TRUE, sm = "fitdistr")
  )

# these two take a long time and error out for some reason
#logcondens = auc_roc_vec(pred = pred, truth = value, smooth = TRUE, sm = "logcondens"),
#logcondens.smooth = auc_roc_vec(pred = pred, truth = value, smooth = TRUE, sm = "logcondens.smooth")

mt <- read_rds("rep_nosmooth/output/model-table-w-results.rds")
mt <- mt %>% left_join(rocs, by = "cell_id")

cols <- c(
  Escalation = "#e41a1c",
  Goldstein = "#ff7f00",
  Quad = "#984ea3",
  CAMEO = "#377eb8",
  Average = "#4daf4a",
  `With PITF Predictors` = "gray50", 
  `Weighted by PITF` = "gray50", 
  `PITF Split Population` = "gray50",
  `PITF Only` = "gray50"
)

model_levels <- c("Escalation", "Quad", "Goldstein", "CAMEO", "Average", 
                  "With PITF Predictors", "Weighted by PITF", "PITF Split Population",
                  "PITF Only")

benefit <- mt %>% 
  mutate(column = fct_recode(column, Escalation = "Escalation Only"),
         column = factor(column, levels = rev(model_levels))) %>%
  mutate(binormal = binormal - no,
         density  = density - no,
         fitdistr = fitdistr - no) %>%
  select(cell_id:non_RF, binormal:fitdistr) %>%
  pivot_longer(binormal:fitdistr, names_to = "method") 

means <- benefit %>%
  group_by(column, method) %>%
  summarize(avg_benefit = mean(value))

ggplot(benefit, aes(x = value, y = column, color = column)) +
  facet_wrap(~method) +
  geom_point() +
  scale_color_manual(guide = FALSE, values = cols) +
  theme(axis.text.y = element_text(size = 12)) +
  geom_point(data = means, aes(x = avg_benefit), shape = "|", color = "black", size = 6) +
  labs(x = "AUC(smoothed ROC) - AUC(empirical ROC)",
       y = "") +
  theme_bw() +
  geom_vline(xintercept = 0, linetype = 3) +
  theme(axis.text.y = element_text(size = 12))
```

